{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic model selection\n",
    "\n",
    "Test notebook for automatic model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "sys.path.append('/Users/davidlaredorazo/Documents/University_of_California/Research/Projects')\n",
    "#sys.path.append('/media/controlslab/DATA/Projects')\n",
    "\n",
    "import ann_framework.aux_functions as aux_functions\n",
    "\n",
    "import automatic_model_selection\n",
    "from automatic_model_selection import Configuration\n",
    "from ann_encoding_rules import Layers\n",
    "import fetch_to_keras\n",
    "#from CMAPSAuxFunctions import TrainValTensorBoard\n",
    "\n",
    "#Tunable model\n",
    "from ann_framework.tunable_model.tunable_model import SequenceTunableModelRegression, SequenceTunableModelClassification\n",
    "\n",
    "#Data handlers\n",
    "from ann_framework.data_handlers.data_handler_CMAPSS import CMAPSSDataHandler\n",
    "from ann_framework.data_handlers.data_handler_MNIST import MNISTDataHandler\n",
    "from ann_framework.data_handlers.data_handler_CIFAR10 import CIFAR10DataHandler\n",
    "\n",
    "learningRate_scheduler = LearningRateScheduler(aux_functions.step_decay)\n",
    "\n",
    "size_scaler = 0.5\n",
    "\n",
    "#Use same configuration for all experiments, just change some of the parameters\n",
    "\n",
    "#Define some random paramaters for the creation of the configuration, this will change for each test model\n",
    "architecture_type = Layers.FullyConnected\n",
    "#architecture_type = Layers.Convolutional\n",
    "problem_type = 2  #1 for regression, 2 for classification\n",
    "output_shape = 10 #If regression applies, number of classes\n",
    "input_shape = (784,)\n",
    "\n",
    "config = Configuration(architecture_type, problem_type, input_shape, output_shape, pop_size=5, \n",
    "                       tournament_size=3, max_similar=3, epochs=5, cross_val=0.2, size_scaler=size_scaler,\n",
    "                       max_generations=1, binary_selection=True, mutation_ratio=0.8, \n",
    "                       similarity_threshold=0.2, more_layers_prob=0.4, verbose_individuals=True, \n",
    "                       show_model=True, verbose_training=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a model, get the compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_compiled_model(model, problem_type, optimizer_params=[]):\n",
    "    \"\"\"Obtain a keras compiled model\"\"\"\n",
    "    \n",
    "    #Shared parameters for the models\n",
    "    optimizer = Adam(lr=0.01, beta_1=0.5)\n",
    "    \n",
    "    if problem_type == 1:\n",
    "        lossFunction = \"mean_squared_error\"\n",
    "        metrics = [\"mse\"]\n",
    "    elif problem_type == 2:\n",
    "        lossFunction = \"categorical_crossentropy\"\n",
    "        metrics = [\"accuracy\"]\n",
    "    else:\n",
    "        print(\"Problem type not defined\")\n",
    "        model = None\n",
    "        return\n",
    "    \n",
    "    #Create and compile the models\n",
    "    model.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_tunable_model(model_genotype, problem_type, input_shape, data_handler, model_number):\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    model = fetch_to_keras.decode_genotype(model_genotype, problem_type, input_shape, 1)\n",
    "    \n",
    "    model = get_compiled_model(model, problem_type, optimizer_params=[])\n",
    "    \n",
    "    if problem_type == 1:\n",
    "        tModel = SequenceTunableModelRegression('ModelReg_SN_'+str(model_number), model, lib_type='keras', data_handler=data_handler)\n",
    "    else:\n",
    "        tModel = SequenceTunableModelClassification('ModelClass_SN_'+str(model_number), model, lib_type='keras', data_handler=data_handler)\n",
    "        \n",
    "    return tModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cmaps data handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cmaps_dhandler():\n",
    "\n",
    "    #Selected as per CNN paper\n",
    "    features = ['T2', 'T24', 'T30', 'T50', 'P2', 'P15', 'P30', 'Nf', 'Nc', 'epr', 'Ps30', 'phi', 'NRf', 'NRc', 'BPR', \n",
    "    'farB', 'htBleed', 'Nf_dmd', 'PCNfR_dmd', 'W31', 'W32']\n",
    "    selected_indices = np.array([2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21])\n",
    "    selected_features = list(features[i] for i in selected_indices-1)\n",
    "    data_folder = '../CMAPSSData'\n",
    "\n",
    "    window_size = 24\n",
    "    window_stride = 1\n",
    "    max_rul = 129\n",
    "\n",
    "    dHandler_cmaps = CMAPSSDataHandler(data_folder, 1, selected_features, max_rul, window_size, window_stride)\n",
    "\n",
    "    input_shape = (len(selected_features)*window_size, )\n",
    "\n",
    "    return dHandler_cmaps, input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to save top models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_best_models(best_models_list, global_best_model_index, saveto, input_shape, data_handler, \n",
    "                     problem_type=1, data_scaler=None, train_epochs=100, metrics=[], round=0):\n",
    "    \n",
    "    n_models = len(best_models_list)\n",
    "    \n",
    "    for ind_model, i in zip(best_models_list, range(n_models)):\n",
    "        \n",
    "        tModel = create_tunable_model(ind_model.stringModel, problem_type, input_shape, data_handler, i)\n",
    "        kmodel = tModel.model\n",
    "        model_json = kmodel.to_json()\n",
    "        \n",
    "        #Save model's architecture\n",
    "        string_append = str(i) if i != global_best_model_index else 'global'\n",
    "        with open(saveto+\"bestModel_\"+string_append+\".json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "            \n",
    "    #Train the global best, model has to be recompiled\n",
    "    ind_model = best_models_list[global_best_model_index]\n",
    "    tModel = create_tunable_model(ind_model.stringModel, problem_type, input_shape, data_handler, n_models)\n",
    "    \n",
    "    print(tModel.model.summary())\n",
    "    print(tModel.data_handler)\n",
    "    \n",
    "    if tModel.data_handler.data_scaler != None:\n",
    "        print(\"Using data handler scaler\")\n",
    "    elif tModel.data_scaler != None:\n",
    "        print(\"Using tModel scaler (Overriding data handler scaler)\")\n",
    "    else:\n",
    "        print(\"No data scaling used\")\n",
    "    \n",
    "    if data_scaler != None:\n",
    "        tModel.data_handler.data_scaler = None\n",
    "        tModel.data_scaler = data_scaler\n",
    "        \n",
    "    tModel.load_data(unroll=True, verbose=1, cross_validation_ratio=0)\n",
    "    tModel.print_data()\n",
    "    tModel.epochs = train_epochs\n",
    "\n",
    "    tModel.train_model(verbose=1)\n",
    "    \n",
    "    tModel.evaluate_model(metrics, round=round)\n",
    "    \n",
    "    kmodel = tModel.model\n",
    "            \n",
    "    # serialize weights to HDF5\n",
    "    kmodel.save_weights(saveto+\"bestModel_global.h5\")\n",
    "    \n",
    "    print(\"Saved models for dataset 1 to disk\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get global best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recompute_globals_fitness(best_models, size_scaler, problem_type):\n",
    "    \"\"\"It is necessary to recompute the fiteness of global models since they have differnt normalization factors\"\"\"\n",
    "\n",
    "    #print(\"Before normalization\")\n",
    "    #automatic_model_selection.print_best(best_models)\n",
    "    \n",
    "    normalize_scores(best_models)\n",
    "    \n",
    "    #print(\"After normalization\")\n",
    "    #automatic_model_selection.print_best(best_models)\n",
    "    \n",
    "    compute_fitness(best_models, size_scaler, problem_type)\n",
    "    \n",
    "    #print(\"Recomputed fitness\")\n",
    "    #automatic_model_selection.print_best(best_models)\n",
    "\n",
    "\n",
    "def normalize_scores(best_models):\n",
    "    \n",
    "    pop_size = len(best_models)\n",
    "    raw_scores = np.zeros((pop_size,))\n",
    "    \n",
    "    for i in range(pop_size):\n",
    "        model = best_models[i]\n",
    "        raw_scores[i] = model.raw_score\n",
    "        \n",
    "    normalization_factor = np.linalg.norm(raw_scores)\n",
    "    normalized_scores = raw_scores/normalization_factor\n",
    "    \n",
    "    for i in range(pop_size):\n",
    "        model = best_models[i]\n",
    "        model.normalized_score = raw_scores[i]\n",
    "    \n",
    "    \n",
    "def compute_fitness(best_models, size_scaler, problem_type):\n",
    "    \n",
    "    pop_size = len(best_models)\n",
    "    \n",
    "    for i in range(pop_size):\n",
    "        \n",
    "        round_up_to = 3\n",
    "\n",
    "        #Round up to the first 3 digits before computing log                                                                                                                                                          \n",
    "        rounding_scaler = 10**round_up_to\n",
    "        trainable_count = round(best_models[i].raw_size/rounding_scaler)*rounding_scaler\n",
    "        size_score = math.log10(trainable_count)\n",
    "\n",
    "        scaled_score = best_models[i].normalized_score\n",
    "\n",
    "        #For classification estimate the error which is between 0 and 1                                                                                                                   \n",
    "        if problem_type == 2:\n",
    "            metric_score = (1 - scaled_score)*10 #Multiply by 10 to have a better scaling. I still need to find an appropriate scaling                                                \n",
    "        else:\n",
    "            metric_score = scaled_score*10 #Multiply by 10 to have a better scaling. I still need to find an appropiate scaling                                                       \n",
    "    \n",
    "        metric_scaler = 1-size_scaler\n",
    "        print(\"metric_scaler %f\"%metric_scaler)\n",
    "        print(\"size scaler %f\"%size_scaler)\n",
    "    \n",
    "        #Scalarization of multiobjective version of the fitness function                                                                                                                  \n",
    "        best_models[i].fitness = metric_scaler*metric_score + size_scaler*size_score\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnist_test(dhandler_mnist, size_scaler=0.5, total_experiments=3):\n",
    "\n",
    "    \"\"\"Input can be of 3 types, ANN (1), CNN (2) or RNN (3)\"\"\"\n",
    "    #architecture_type = Layers.FullyConnected\n",
    "    architecture_type = Layers.Convolutional\n",
    "    problem_type = 2  #1 for regression, 2 for classification\n",
    "    output_shape = 10 #If regression applies, number of classes\n",
    "    #input_shape = (784,)\n",
    "    input_shape = (28,28,1)\n",
    "    unroll = False\n",
    "\n",
    "    \"\"\"\n",
    "    pop_size = 5\n",
    "    tournament_size = 3\n",
    "    max_similar = 3\n",
    "    total_experiments = 5\n",
    "    count_experiments = 0\n",
    "    unroll = True\n",
    "    \"\"\"\n",
    "    #total_experiments = 1\n",
    "    count_experiments = 0\n",
    "\n",
    "    global_best_list = []\n",
    "    global_best = None\n",
    "    global_best_index = 0\n",
    "    experiment_times = np.zeros((total_experiments,1))\n",
    "\n",
    "    scaler = None\n",
    "\n",
    "    t = datetime.datetime.now()\n",
    "\n",
    "    logging.basicConfig(filename='logs/nn_evolution_mnist_' + t.strftime('%m%d%Y%H%M%S') + '.log', level=logging.INFO, \n",
    "                            format='%(levelname)s:%(threadName)s:%(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "    #mnist datahandler\n",
    "    #dHandler_mnist = MNISTDataHandler()\n",
    "\n",
    "    config.architecture_type = architecture_type\n",
    "    config.problem_type = problem_type\n",
    "    config.input_shape = input_shape\n",
    "    config.output_shape = output_shape\n",
    "    config.size_scaler = size_scaler\n",
    "\n",
    "    \"\"\"\n",
    "    config = Configuration(architecture_type, problem_type, input_shape, output_shape, pop_size, \n",
    "                           tournament_size, max_similar, epochs=20, cross_val=0.2, size_scaler=size_scaler,\n",
    "                           max_generations=10, binary_selection=True, mutation_ratio=0.4, \n",
    "                           similarity_threshold=0.2, more_layers_prob=0.8)\n",
    "    \"\"\"\n",
    "\n",
    "    while count_experiments < total_experiments:\n",
    "        print(\"Launching experiment {}\".format(count_experiments+1))\n",
    "        logging.info(\"Launching experiment {}\".format(count_experiments+1))\n",
    "\n",
    "\n",
    "        start = time.time()\n",
    "        best = automatic_model_selection.run_experiment(config, dHandler_mnist, count_experiments + 1, unroll=unroll,\n",
    "                                                        learningRate_scheduler=learningRate_scheduler, \n",
    "                                                        tModel_scaler=scaler)\n",
    "        end = time.time()\n",
    "        elapsed_time = (end-start)/60\n",
    "        experiment_times[count_experiments] = elapsed_time\n",
    "        print(\"Experiment time: {} minutes\".format(elapsed_time))\n",
    "        logging.info(\"Experiment time: {} minutes\".format(elapsed_time))\n",
    "\n",
    "\n",
    "        best.individual_label = count_experiments\n",
    "\n",
    "        global_best_list.append(best)\n",
    "        \n",
    "        \"\"\"\n",
    "        if global_best == None:\n",
    "            global_best = best\n",
    "        else:\n",
    "            if best.fitness < global_best.fitness:\n",
    "                global_best = best\n",
    "                global_best_index = count_experiments\n",
    "        \"\"\"\n",
    "\n",
    "        count_experiments =  count_experiments + 1\n",
    "    \n",
    "    recompute_globals_fitness(global_best_list, config.size_scaler, config.problem_type)\n",
    "    \n",
    "    total_experiment_time = experiment_times.sum()\n",
    "\n",
    "    print(\"Global best list\\n\")\n",
    "    logging.info(\"Global best list\\n\")\n",
    "    automatic_model_selection.print_best(global_best_list)\n",
    "\n",
    "    print(\"Global best is\\n\")\n",
    "    print(global_best)\n",
    "    logging.info(\"Global best is\\n\")\n",
    "    logging.info(global_best)\n",
    "\n",
    "    print(\"Global time {}\".format(experiment_times.sum()))\n",
    "    logging.info(\"Global time {}\".format(experiment_times.sum()))\n",
    "    \n",
    "    return global_best_list, global_best_index, total_experiment_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cifar_test(dHandler_cifar, size_scaler=0.5, total_experiments=3):\n",
    "\n",
    "    \"\"\"Input can be of 3 types, ANN (1), CNN (2) or RNN (3)\"\"\"\n",
    "    architecture_type = Layers.FullyConnected\n",
    "    problem_type = 2  #1 for regression, 2 for classification\n",
    "    output_shape = 10 #If regression applies, number of classes\n",
    "    input_shape = (3072,)\n",
    "    \"\"\"\n",
    "    pop_size = 5\n",
    "    tournament_size = 3\n",
    "    max_similar = 3\n",
    "    \"\"\"\n",
    "    total_experiments = 5\n",
    "    count_experiments = 0\n",
    "    unroll = True\n",
    "\n",
    "    global_best_list = []\n",
    "    global_best = None\n",
    "    global_best_index = 0\n",
    "\n",
    "    scaler = None\n",
    "\n",
    "    t = datetime.datetime.now()\n",
    "\n",
    "    logging.basicConfig(filename='logs/nn_evolution_cifar10_' + t.strftime('%m%d%Y%H%M%S') + '.log', level=logging.INFO, \n",
    "                            format='%(levelname)s:%(threadName)s:%(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "    #mnist datahandler\n",
    "    #dHandler_cifar = CIFAR10DataHandler()\n",
    "\n",
    "    \"\"\"\n",
    "    config = Configuration(architecture_type, problem_type, input_shape, output_shape, pop_size, tournament_size, max_similar, \n",
    "                           epochs=5, cross_val=0.2, size_scaler=size_scaler, max_generations=10, binary_selection=True, \n",
    "                           mutation_ratio=0.4, similarity_threshold=0.2, more_layers_prob=0.8)\n",
    "    \"\"\"\n",
    "\n",
    "    config.architecture_type = architecture_type\n",
    "    config.problem_type = problem_type\n",
    "    config.input_shape = input_shape\n",
    "    config.output_shape = output_shape\n",
    "\n",
    "    while count_experiments < total_experiments:\n",
    "        print(\"Launching experiment {}\".format(count_experiments+1))\n",
    "        logging.info(\"Launching experiment {}\".format(count_experiments+1))\n",
    "\n",
    "        best = automatic_model_selection.run_experiment(config, dHandler_cifar, count_experiments + 1, unroll=unroll,\n",
    "                                                        learningRate_scheduler=learningRate_scheduler, \n",
    "                                                        tModel_scaler=scaler, verbose_data=0)\n",
    "\n",
    "        best.individual_label = count_experiments\n",
    "\n",
    "        global_best_list.append(best)\n",
    "\n",
    "        if global_best == None:\n",
    "            global_best = best\n",
    "        else:\n",
    "            if best.fitness < global_best.fitness:\n",
    "                global_best = best\n",
    "                global_best_index = count_experiments\n",
    "\n",
    "        count_experiments =  count_experiments + 1\n",
    "        \n",
    "    total_experiment_time = experiment_times.sum()\n",
    "\n",
    "    print(\"Global best list\\n\")\n",
    "    logging.info(\"Global best list\\n\")\n",
    "    automatic_model_selection.print_best(global_best_list)\n",
    "\n",
    "    print(\"Global best is\\n\")\n",
    "    print(global_best)\n",
    "    logging.info(\"Global best is\\n\")\n",
    "    logging.info(global_best)\n",
    "    \n",
    "    return global_best_list, global_best_index, total_experiment_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test on CMAPSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cmapss_test(dhandler_cmaps, input_shape, size_scaler=0.5, total_experiments=3):\n",
    "\n",
    "    \"\"\"Input can be of 3 types, ANN (1), CNN (2) or RNN (3)\"\"\"\n",
    "    architecture_type = Layers.FullyConnected\n",
    "    problem_type = 1  #1 for regression, 2 for classification\n",
    "    output_shape = 1 #If regression applies, number of classes\n",
    "    input_shape = None\n",
    "\n",
    "    \"\"\"\n",
    "    pop_size = 5\n",
    "    tournament_size = 3\n",
    "    max_similar = 3\n",
    "    \"\"\"\n",
    "    total_experiments = 5\n",
    "    count_experiments = 0\n",
    "    unroll = True\n",
    "\n",
    "    global_best_list = []\n",
    "    global_best = None\n",
    "    global_best_index = 0\n",
    "    experiment_times = np.zeros((total_experiments,1))\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "    t = datetime.datetime.now()\n",
    "\n",
    "    logging.basicConfig(filename='logs/nn_evolution_cmaps_' + t.strftime('%m%d%Y%H%M%S') + '.log', level=logging.INFO, \n",
    "                            format='%(levelname)s:%(threadName)s:%(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "    #cmaps datahandler\n",
    "    #dhandler_cmaps, input_shape = cmaps_dhandler()\n",
    "\n",
    "    \"\"\"\n",
    "    config = Configuration(architecture_type, problem_type, input_shape, output_shape, pop_size, tournament_size, \n",
    "                           max_similar, epochs=5, cross_val=0.2, size_scaler=size_scaler, max_generations=10, \n",
    "                           binary_selection=True, mutation_ratio=0.4, similarity_threshold=0.2, more_layers_prob=0.8)\n",
    "    \"\"\"\n",
    "\n",
    "    config.architecture_type = architecture_type\n",
    "    config.problem_type = problem_type\n",
    "    config.input_shape = input_shape\n",
    "    config.output_shape = output_shape\n",
    "\n",
    "    while count_experiments < total_experiments:\n",
    "        print(\"Launching experiment {}\".format(count_experiments+1))\n",
    "        logging.info(\"Launching experiment {}\".format(count_experiments+1))\n",
    "\n",
    "        start = time.time()\n",
    "        best = automatic_model_selection.run_experiment(config, dhandler_cmaps, count_experiments + 1, unroll=unroll,\n",
    "                                                        learningRate_scheduler=learningRate_scheduler, \n",
    "                                                        tModel_scaler=scaler)\n",
    "        end = time.time()\n",
    "        elapsed_time = (end-start)/60\n",
    "        experiment_times[count_experiments] = elapsed_time\n",
    "        print(\"Experiment time: {} minutes\".format(elapsed_time))\n",
    "        logging.info(\"Experiment time: {} minutes\".format(elapsed_time))\n",
    "\n",
    "        best.individual_label = count_experiments\n",
    "\n",
    "        global_best_list.append(best)\n",
    "\n",
    "        if global_best == None:\n",
    "            global_best = best\n",
    "        else:\n",
    "            if best.fitness < global_best.fitness:\n",
    "                global_best = best\n",
    "                global_best_index = count_experiments\n",
    "\n",
    "        count_experiments =  count_experiments + 1\n",
    "        \n",
    "    total_experiment_time = experiment_times.sum()\n",
    "\n",
    "    print(\"Global best list\\n\")\n",
    "    logging.info(\"Global best list\\n\")\n",
    "    automatic_model_selection.print_best(global_best_list)\n",
    "\n",
    "    print(\"Global best is\\n\")\n",
    "    print(global_best)\n",
    "    logging.info(\"Global best is\\n\")\n",
    "    logging.info(global_best)\n",
    "\n",
    "    print(\"Global time {}\".format(total_experiment_time))\n",
    "    logging.info(\"Global time {}\".format(total_experiment_time))\n",
    "    \n",
    "    return global_best_list, global_best_index, total_experiment_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for alpha=0.2\n",
      "Launching experiment 1\n",
      "Generating layer limits according to dataset\n",
      "(28, 28, 1)\n",
      "Loading data. Cross-Validation ratio 0\n",
      "28\n",
      "{}\n",
      "Previous layer\n",
      "None\n",
      "Layer limits\n",
      "{'max_neuron_multiplier': 128, 'max_filter_size_multiplier': 64, 'max_kernel_size_multiplier': 14, 'max_filter_stride': 3, 'max_pooling_exponent': 6, 'max_dropout': 0.7}\n",
      "Previous Layer\n",
      "None\n",
      "generating convolutional layer\n",
      "filter size 80\n",
      "previous size 14\n",
      "max kernel size 7 \n",
      "max kernel size multiplier 7 \n",
      "Conv layer generated\n",
      "[<Layers.Convolutional: 2>, 0, 1, 80, 5, 3, 0, 0, 0, 0]\n",
      "{<Layers.Convolutional: 2>: 1}\n",
      "Previous layer\n",
      "{<Layers.Convolutional: 2>: 1}\n",
      "Previous layer\n",
      "{<Layers.Convolutional: 2>: 1}\n",
      "{}\n",
      "Previous layer\n",
      "None\n",
      "Layer limits\n",
      "{'max_neuron_multiplier': 128, 'max_filter_size_multiplier': 64, 'max_kernel_size_multiplier': 14, 'max_filter_stride': 3, 'max_pooling_exponent': 6, 'max_dropout': 0.7}\n",
      "Previous Layer\n",
      "None\n",
      "generating convolutional layer\n",
      "filter size 504\n",
      "previous size 14\n",
      "max kernel size 7 \n",
      "max kernel size multiplier 7 \n",
      "Conv layer generated\n",
      "[<Layers.Convolutional: 2>, 0, 1, 504, 18, 1, 0, 0, 0, 0]\n",
      "{<Layers.Convolutional: 2>: 1}\n",
      "Previous layer\n",
      "{<Layers.Convolutional: 2>: 1}\n",
      "Layer limits\n",
      "{'max_neuron_multiplier': 128, 'max_filter_size_multiplier': 64, 'max_kernel_size_multiplier': 14, 'max_filter_stride': 3, 'max_pooling_exponent': 6, 'max_dropout': 0.7}\n",
      "Previous Layer\n",
      "{<Layers.Convolutional: 2>: 1}\n",
      "generating convolutional layer\n",
      "filter size 184\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-50df1157d97d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     global_best_list, global_best_index, total_experiment_time = mnist_test(dHandler_mnist, \n\u001b[1;32m     18\u001b[0m                                                                             \u001b[0msize_scaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_scaler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                                                             total_experiments=experiments)\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mavg_experiment_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_experiment_time\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mexperiments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-4d7e15b7cdd7>\u001b[0m in \u001b[0;36mmnist_test\u001b[0;34m(dhandler_mnist, size_scaler, total_experiments)\u001b[0m\n\u001b[1;32m     57\u001b[0m         best = automatic_model_selection.run_experiment(config, dHandler_mnist, count_experiments + 1, unroll=unroll,\n\u001b[1;32m     58\u001b[0m                                                         \u001b[0mlearningRate_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearningRate_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                                                         tModel_scaler=scaler)\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/University_of_California/Research/Projects/automatic_model_selection/code/automatic_model_selection.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(configuration, data_handler, experiment_number, unroll, learningRate_scheduler, tModel_scaler)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \tpopulation = nn_evolutionary.initial_population(configuration.pop_size, configuration.problem_type, configuration.architecture_type, layer_limits,\n\u001b[0;32m--> 431\u001b[0;31m \t\t\t\t\t\t\t\t\t\t\t\t\tnumber_classes=configuration.output_shape, more_layers_prob=configuration.more_layers_prob)\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/University_of_California/Research/Projects/automatic_model_selection/code/nn_evolutionary.py\u001b[0m in \u001b[0;36minitial_population\u001b[0;34m(pop_size, problem_type, architecture_type, layer_limits, number_classes, more_layers_prob)\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mused_activations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0mmodel_genotype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_limits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmore_layers_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmore_layers_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_component\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_first\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused_activations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mused_activations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0;31m#Last layer is always FC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/University_of_California/Research/Projects/automatic_model_selection/code/nn_evolutionary.py\u001b[0m in \u001b[0;36mgenerate_model\u001b[0;34m(layer_limits, model, prev_component, next_component, max_layers, more_layers_prob, used_activations)\u001b[0m\n\u001b[1;32m    312\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                         \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mann_encoding_rules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_component\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer_limits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused_activations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                         \u001b[0mprev_component\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/University_of_California/Research/Projects/automatic_model_selection/code/ann_encoding_rules.py\u001b[0m in \u001b[0;36mgenerate_layer\u001b[0;34m(layer_type, layer_limits, previous_layer, used_activations)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m#If layer is convolutional try to generate it first, if its not possible then replace by fully connected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlayer_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConvolutional\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                 \u001b[0mconvLayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_convolutional_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_limits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mconvLayer\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/University_of_California/Research/Projects/automatic_model_selection/code/ann_encoding_rules.py\u001b[0m in \u001b[0;36mgenerate_convolutional_layer\u001b[0;34m(previous_layer, layer_limits)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_layer\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                 \u001b[0mprevious_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mprevious_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_kernel_size_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 8"
     ]
    }
   ],
   "source": [
    "#alphas = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "alphas = [0.2]\n",
    "experiments = 3\n",
    "\n",
    "global_best_list = []\n",
    "global_best_index = 0\n",
    "total_experiment_time = []\n",
    "total_experiment_time = 0\n",
    "avg_experiment_time = 0\n",
    "\n",
    "dHandler_mnist = MNISTDataHandler()\n",
    "\n",
    "for size_scaler in alphas:\n",
    "\n",
    "    print(\"Running for alpha={}\".format(size_scaler))\n",
    "    \n",
    "    global_best_list, global_best_index, total_experiment_time = mnist_test(dHandler_mnist, \n",
    "                                                                            size_scaler=size_scaler, \n",
    "                                                                            total_experiments=experiments)\n",
    "    \n",
    "    avg_experiment_time = total_experiment_time/experiments\n",
    "    \n",
    "    print(\"Total experiment time {}\".format(total_experiment_time))\n",
    "    print(\"Avg experiment time {}\".format(avg_experiment_time))\n",
    "    \n",
    "    save_best_models(global_best_list, global_best_index, \n",
    "                     'best_models/mnist/scalarized/alpha{}/'.format(size_scaler), input_shape=input_shape, \n",
    "                     data_handler=dHandler_mnist, problem_type=problem_type, train_epochs=100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "dHandler_cifar = CIFAR10DataHandler()\n",
    "\n",
    "global_best_list, global_best_index = cifar_test(dHandler_cifar, size_scaler=size_scaler, total_experiments=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dhandler_cmaps, input_shape = cmaps_dhandler()\n",
    "\n",
    "global_best_list, global_best_index = cmapss_test(dhandler_cmaps, input_shape, size_scaler=size_scaler, total_experiments=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "save_best_models(global_best_list, global_best_index, 'best_models/mnist/alpha{}/scalarized_version/'.format(size_scaler), \n",
    "                 input_shape=input_shape, data_handler=dHandler_mnist, problem_type=problem_type, train_epochs=100)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "save_best_models(global_best_list, global_best_index, 'best_models/cifar10/alpha{}/version2'.format(size_scaler), \n",
    "                 input_shape=input_shape, data_handler=dHandler_cifar, problem_type=problem_type, train_epochs=100)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "save_best_models(global_best_list, global_best_index, 'best_models/cmapss/alpha{}/'.format(size_scaler), \n",
    "                 input_shape=input_shape, data_handler=dhandler_cmaps, problem_type=problem_type, train_epochs=100, \n",
    "                 data_scaler=scaler, metrics=['rhs', 'rmse'], round=2)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
