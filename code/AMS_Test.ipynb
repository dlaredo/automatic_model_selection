{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Model Selection\n",
    "\n",
    "Test notebook for automatic model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "#sys.path.append('/Users/davidlaredorazo/Documents/University_of_California/Research/Projects')\n",
    "sys.path.append('/media/controlslab/DATA/Projects')\n",
    "\n",
    "import ann_framework.aux_functions as aux_functions\n",
    "\n",
    "import automatic_model_selection\n",
    "from automatic_model_selection import Configuration\n",
    "from ann_encoding_rules import Layers\n",
    "import fetch_to_keras\n",
    "#from CMAPSAuxFunctions import TrainValTensorBoard\n",
    "\n",
    "#Tunable model\n",
    "from ann_framework.tunable_model.tunable_model import SequenceTunableModelRegression, SequenceTunableModelClassification\n",
    "\n",
    "#Data handlers\n",
    "from ann_framework.data_handlers.data_handler_CMAPSS import CMAPSSDataHandler\n",
    "from ann_framework.data_handlers.data_handler_MNIST import MNISTDataHandler\n",
    "from ann_framework.data_handlers.data_handler_CIFAR10 import CIFAR10DataHandler\n",
    "\n",
    "learningRate_scheduler = LearningRateScheduler(aux_functions.step_decay)\n",
    "\n",
    "size_scaler = 0.5\n",
    "\n",
    "#Use same configuration for all experiments, just change some of the parameters\n",
    "\n",
    "#Define some random paramaters for the creation of the configuration, this will change for each test model\n",
    "architecture_type = Layers.FullyConnected\n",
    "#architecture_type = Layers.Convolutional\n",
    "problem_type = 2  #1 for regression, 2 for classification\n",
    "output_shape = 10 #If regression applies, number of classes\n",
    "input_shape = (784,)\n",
    "\n",
    "config = Configuration(architecture_type, problem_type, input_shape, output_shape, pop_size=5, \n",
    "                       tournament_size=3, max_similar=3, epochs=5, cross_val=0.2, size_scaler=size_scaler,\n",
    "                       max_generations=10, binary_selection=True, mutation_ratio=0.8, \n",
    "                       similarity_threshold=0.2, more_layers_prob=0.4, verbose_individuals=True, \n",
    "                       show_model=True, verbose_training=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given a model get the compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model(model, problem_type, optimizer_params=[]):\n",
    "    \"\"\"Obtain a keras compiled model\"\"\"\n",
    "    \n",
    "    #Shared parameters for the models\n",
    "    optimizer = Adam(lr=0.01, beta_1=0.5)\n",
    "    \n",
    "    if problem_type == 1:\n",
    "        lossFunction = \"mean_squared_error\"\n",
    "        metrics = [\"mse\"]\n",
    "    elif problem_type == 2:\n",
    "        lossFunction = \"categorical_crossentropy\"\n",
    "        metrics = [\"accuracy\"]\n",
    "    else:\n",
    "        print(\"Problem type not defined\")\n",
    "        model = None\n",
    "        return\n",
    "    \n",
    "    #Create and compile the models\n",
    "    model.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_tunable_model(model_genotype, problem_type, input_shape, data_handler, model_number):\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    model = fetch_to_keras.decode_genotype(model_genotype, problem_type, input_shape, 1)\n",
    "    \n",
    "    model = get_compiled_model(model, problem_type, optimizer_params=[])\n",
    "    \n",
    "    if problem_type == 1:\n",
    "        tModel = SequenceTunableModelRegression('ModelReg_SN_'+str(model_number), model, lib_type='keras', data_handler=data_handler)\n",
    "    else:\n",
    "        tModel = SequenceTunableModelClassification('ModelClass_SN_'+str(model_number), model, lib_type='keras', data_handler=data_handler)\n",
    "        \n",
    "    return tModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the CMAPSS datahandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmapss_dhandler():\n",
    "\n",
    "    #Selected as per CNN paper\n",
    "    features = ['T2', 'T24', 'T30', 'T50', 'P2', 'P15', 'P30', 'Nf', 'Nc', 'epr', 'Ps30', 'phi', 'NRf', 'NRc', 'BPR', \n",
    "    'farB', 'htBleed', 'Nf_dmd', 'PCNfR_dmd', 'W31', 'W32']\n",
    "    selected_indices = np.array([2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21])\n",
    "    selected_features = list(features[i] for i in selected_indices-1)\n",
    "    data_folder = '../CMAPSSData'\n",
    "\n",
    "    window_size = 24\n",
    "    window_stride = 1\n",
    "    max_rul = 129\n",
    "\n",
    "    dhandler_cmapss = CMAPSSDataHandler(data_folder, 1, selected_features, max_rul, window_size, window_stride)\n",
    "\n",
    "    input_shape = (len(selected_features)*window_size, )\n",
    "\n",
    "    return dhandler_cmapss, input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to save top models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_models(best_models_list, global_best_model_index, saveto, input_shape, data_handler, \n",
    "                     problem_type=1, data_scaler=None, train_epochs=100, metrics=[], round=0):\n",
    "    \n",
    "    n_models = len(best_models_list)\n",
    "    \n",
    "    for ind_model, i in zip(best_models_list, range(n_models)):\n",
    "        \n",
    "        tModel = create_tunable_model(ind_model.stringModel, problem_type, input_shape, data_handler, i)\n",
    "        kmodel = tModel.model\n",
    "        model_json = kmodel.to_json()\n",
    "        \n",
    "        #Save model's architecture\n",
    "        string_append = str(i) if i != global_best_model_index else 'global'\n",
    "        with open(saveto+\"bestModel_\"+string_append+\".json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "            \n",
    "    #Train the global best, model has to be recompiled\n",
    "    ind_model = best_models_list[global_best_model_index]\n",
    "    tModel = create_tunable_model(ind_model.stringModel, problem_type, input_shape, data_handler, n_models)\n",
    "    \n",
    "    print(tModel.model.summary())\n",
    "    print(tModel.data_handler)\n",
    "    \n",
    "    if tModel.data_handler.data_scaler != None:\n",
    "        print(\"Using data handler scaler\")\n",
    "    elif tModel.data_scaler != None:\n",
    "        print(\"Using tModel scaler (Overriding data handler scaler)\")\n",
    "    else:\n",
    "        print(\"No data scaling used\")\n",
    "    \n",
    "    if data_scaler != None:\n",
    "        tModel.data_handler.data_scaler = None\n",
    "        tModel.data_scaler = data_scaler\n",
    "        \n",
    "    tModel.load_data(unroll=True, verbose=1, cross_validation_ratio=0)\n",
    "    tModel.print_data()\n",
    "    tModel.epochs = train_epochs\n",
    "\n",
    "    tModel.train_model(verbose=1)\n",
    "    \n",
    "    tModel.evaluate_model(metrics, round=round)\n",
    "    \n",
    "    kmodel = tModel.model\n",
    "            \n",
    "    # serialize weights to HDF5\n",
    "    kmodel.save_weights(saveto+\"bestModel_global.h5\")\n",
    "    \n",
    "    print(\"Saved models for dataset 1 to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get global best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_globals_fitness(best_models, size_scaler, problem_type):\n",
    "    \"\"\"It is necessary to recompute the fiteness of global models since they have differnt normalization factors\"\"\"\n",
    "\n",
    "    #print(\"Before normalization\")\n",
    "    #automatic_model_selection.print_best(best_models)\n",
    "    \n",
    "    normalize_scores(best_models)\n",
    "    \n",
    "    #print(\"After normalization\")\n",
    "    #automatic_model_selection.print_best(best_models)\n",
    "    \n",
    "    global_best_index = compute_fitness(best_models, size_scaler, problem_type)\n",
    "    \n",
    "    print(\"Recomputed fitness\")\n",
    "    automatic_model_selection.print_best(best_models)\n",
    "    print(\"Global best index\")\n",
    "    print(global_best_index)\n",
    "    \n",
    "    return global_best_index\n",
    "\n",
    "\n",
    "def normalize_scores(best_models):\n",
    "    \n",
    "    pop_size = len(best_models)\n",
    "    raw_scores = np.zeros((pop_size,))\n",
    "    \n",
    "    for i in range(pop_size):\n",
    "        model = best_models[i]\n",
    "        raw_scores[i] = model.raw_score\n",
    "        \n",
    "    normalization_factor = np.linalg.norm(raw_scores)\n",
    "    normalized_scores = raw_scores/normalization_factor\n",
    "    \n",
    "    for i in range(pop_size):\n",
    "        model = best_models[i]\n",
    "        model.normalized_score = raw_scores[i]\n",
    "    \n",
    "    \n",
    "def compute_fitness(best_models, size_scaler, problem_type):\n",
    "    \n",
    "    pop_size = len(best_models)\n",
    "    \n",
    "    global_best_index = 0\n",
    "    \n",
    "    for i in range(pop_size):\n",
    "        \n",
    "        round_up_to = 3\n",
    "\n",
    "        #Round up to the first 3 digits before computing log                                                                                                                                                          \n",
    "        rounding_scaler = 10**round_up_to\n",
    "        trainable_count = round(best_models[i].raw_size/rounding_scaler)*rounding_scaler\n",
    "        size_score = math.log10(trainable_count)\n",
    "\n",
    "        scaled_score = best_models[i].normalized_score\n",
    "\n",
    "        #For classification estimate the error which is between 0 and 1                                                                                                                   \n",
    "        if problem_type == 2:\n",
    "            metric_score = (1 - scaled_score)*10 #Multiply by 10 to have a better scaling. I still need to find an appropriate scaling                                                \n",
    "        else:\n",
    "            metric_score = scaled_score*10 #Multiply by 10 to have a better scaling. I still need to find an appropiate scaling                                                       \n",
    "    \n",
    "        metric_scaler = 1-size_scaler\n",
    "        print(\"metric_scaler %f\"%metric_scaler)\n",
    "        print(\"size scaler %f\"%size_scaler)\n",
    "    \n",
    "        #Scalarization of multiobjective version of the fitness function                                                                                                                  \n",
    "        best_models[i].fitness = metric_scaler*metric_score + size_scaler*size_score\n",
    "        \n",
    "        if best_models[i].fitness < best_models[global_best_index].fitness:\n",
    "            global_best_index = i\n",
    "            \n",
    "    return global_best_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_test(dhandler_mnist, size_scaler=0.5, total_experiments=3):\n",
    "\n",
    "    \"\"\"Input can be of 3 types, ANN (1), CNN (2) or RNN (3)\"\"\"\n",
    "    architecture_type = Layers.FullyConnected\n",
    "    #architecture_type = Layers.Convolutional\n",
    "    problem_type = 2  #1 for regression, 2 for classification\n",
    "    output_shape = 10 #If regression applies, number of classes\n",
    "    input_shape = (784,)\n",
    "    #input_shape = (28,28,1)\n",
    "\n",
    "    \"\"\"\n",
    "    pop_size = 5\n",
    "    tournament_size = 3\n",
    "    max_similar = 3\n",
    "    total_experiments = 5\n",
    "    count_experiments = 0\n",
    "    unroll = True\n",
    "    \"\"\"\n",
    "    #total_experiments = 1\n",
    "    count_experiments = 0\n",
    "    unroll = True\n",
    "\n",
    "    global_best_list = []\n",
    "    global_best = None\n",
    "    global_best_index = 0\n",
    "    experiment_times = np.zeros((total_experiments,1))\n",
    "\n",
    "    scaler = None\n",
    "\n",
    "    t = datetime.datetime.now()\n",
    "    \n",
    "    #Clear logging facility before attempting to create log\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    \n",
    "    logging.basicConfig(filename='logs/nn_evolution_mnist_' + t.strftime('%m%d%Y%H%M%S') + '.log', level=logging.INFO, \n",
    "                            format='%(levelname)s:%(threadName)s:%(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "    #mnist datahandler\n",
    "    #dHandler_mnist = MNISTDataHandler()\n",
    "\n",
    "    config.architecture_type = architecture_type\n",
    "    config.problem_type = problem_type\n",
    "    config.input_shape = input_shape\n",
    "    config.output_shape = output_shape\n",
    "    config.size_scaler = size_scaler\n",
    "    config.epochs=5\n",
    "\n",
    "    \"\"\"\n",
    "    config = Configuration(architecture_type, problem_type, input_shape, output_shape, pop_size, \n",
    "                           tournament_size, max_similar, epochs=20, cross_val=0.2, size_scaler=size_scaler,\n",
    "                           max_generations=10, binary_selection=True, mutation_ratio=0.4, \n",
    "                           similarity_threshold=0.2, more_layers_prob=0.8)\n",
    "    \"\"\"\n",
    "\n",
    "    while count_experiments < total_experiments:\n",
    "        print(\"Launching experiment {}\".format(count_experiments+1))\n",
    "        logging.info(\"Launching experiment {}\".format(count_experiments+1))\n",
    "\n",
    "\n",
    "        start = time.time()\n",
    "        best = automatic_model_selection.run_experiment(config, dHandler_mnist, count_experiments + 1, unroll=unroll,\n",
    "                                                        learningRate_scheduler=learningRate_scheduler, \n",
    "                                                        tModel_scaler=scaler)\n",
    "        end = time.time()\n",
    "        elapsed_time = (end-start)/60\n",
    "        experiment_times[count_experiments] = elapsed_time\n",
    "        print(\"Experiment time: {} minutes\".format(elapsed_time))\n",
    "        logging.info(\"Experiment time: {} minutes\".format(elapsed_time))\n",
    "\n",
    "\n",
    "        best.individual_label = count_experiments\n",
    "\n",
    "        global_best_list.append(best)\n",
    "        \n",
    "        \"\"\"\n",
    "        if global_best == None:\n",
    "            global_best = best\n",
    "        else:\n",
    "            if best.fitness < global_best.fitness:\n",
    "                global_best = best\n",
    "                global_best_index = count_experiments\n",
    "        \"\"\"\n",
    "\n",
    "        count_experiments =  count_experiments + 1\n",
    "    \n",
    "    print(\"Recompute globals fitness\")\n",
    "    global_best_index = recompute_globals_fitness(global_best_list, config.size_scaler, config.problem_type)\n",
    "    global_best = global_best_list[global_best_index]\n",
    "    \n",
    "    total_experiment_time = experiment_times.sum()\n",
    "\n",
    "    print(\"Global best list\\n\")\n",
    "    logging.info(\"Global best list\\n\")\n",
    "    automatic_model_selection.print_best(global_best_list)\n",
    "\n",
    "    print(\"Global best is\\n\")\n",
    "    print(global_best)\n",
    "    logging.info(\"Global best is\\n\")\n",
    "    logging.info(global_best)\n",
    "\n",
    "    print(\"Global time {}\".format(experiment_times.sum()))\n",
    "    logging.info(\"Global time {}\".format(experiment_times.sum()))\n",
    "    \n",
    "    logging.shutdown()\n",
    "    \n",
    "    return global_best_list, global_best_index, total_experiment_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_test(dHandler_cifar, size_scaler=0.5, total_experiments=3):\n",
    "\n",
    "    \"\"\"Input can be of 3 types, ANN (1), CNN (2) or RNN (3)\"\"\"\n",
    "    architecture_type = Layers.FullyConnected\n",
    "    problem_type = 2  #1 for regression, 2 for classification\n",
    "    output_shape = 10 #If regression applies, number of classes\n",
    "    input_shape = (3072,)\n",
    "    \"\"\"\n",
    "    pop_size = 5\n",
    "    tournament_size = 3\n",
    "    max_similar = 3\n",
    "    \"\"\"\n",
    "    total_experiments = 5\n",
    "    count_experiments = 0\n",
    "    unroll = True\n",
    "\n",
    "    global_best_list = []\n",
    "    global_best = None\n",
    "    global_best_index = 0\n",
    "\n",
    "    scaler = None\n",
    "\n",
    "    t = datetime.datetime.now()\n",
    "\n",
    "    logging.basicConfig(filename='logs/nn_evolution_cifar10_' + t.strftime('%m%d%Y%H%M%S') + '.log', level=logging.INFO, \n",
    "                            format='%(levelname)s:%(threadName)s:%(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "    #mnist datahandler\n",
    "    #dHandler_cifar = CIFAR10DataHandler()\n",
    "\n",
    "    \"\"\"\n",
    "    config = Configuration(architecture_type, problem_type, input_shape, output_shape, pop_size, tournament_size, max_similar, \n",
    "                           epochs=5, cross_val=0.2, size_scaler=size_scaler, max_generations=10, binary_selection=True, \n",
    "                           mutation_ratio=0.4, similarity_threshold=0.2, more_layers_prob=0.8)\n",
    "    \"\"\"\n",
    "\n",
    "    config.architecture_type = architecture_type\n",
    "    config.problem_type = problem_type\n",
    "    config.input_shape = input_shape\n",
    "    config.output_shape = output_shape\n",
    "\n",
    "    while count_experiments < total_experiments:\n",
    "        print(\"Launching experiment {}\".format(count_experiments+1))\n",
    "        logging.info(\"Launching experiment {}\".format(count_experiments+1))\n",
    "\n",
    "        best = automatic_model_selection.run_experiment(config, dHandler_cifar, count_experiments + 1, unroll=unroll,\n",
    "                                                        learningRate_scheduler=learningRate_scheduler, \n",
    "                                                        tModel_scaler=scaler, verbose_data=0)\n",
    "\n",
    "        best.individual_label = count_experiments\n",
    "\n",
    "        global_best_list.append(best)\n",
    "\n",
    "        if global_best == None:\n",
    "            global_best = best\n",
    "        else:\n",
    "            if best.fitness < global_best.fitness:\n",
    "                global_best = best\n",
    "                global_best_index = count_experiments\n",
    "\n",
    "        count_experiments =  count_experiments + 1\n",
    "        \n",
    "    total_experiment_time = experiment_times.sum()\n",
    "\n",
    "    print(\"Global best list\\n\")\n",
    "    logging.info(\"Global best list\\n\")\n",
    "    automatic_model_selection.print_best(global_best_list)\n",
    "\n",
    "    print(\"Global best is\\n\")\n",
    "    print(global_best)\n",
    "    logging.info(\"Global best is\\n\")\n",
    "    logging.info(global_best)\n",
    "    \n",
    "    return global_best_list, global_best_index, total_experiment_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on CMAPSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmapss_test(dhandler_cmapss, input_shape, size_scaler=0.5, total_experiments=3):\n",
    "\n",
    "    \"\"\"Input can be of 3 types, ANN (1), CNN (2) or RNN (3)\"\"\"\n",
    "    architecture_type = Layers.FullyConnected\n",
    "    problem_type = 1  #1 for regression, 2 for classification\n",
    "    output_shape = 1 #If regression applies, number of classes\n",
    "\n",
    "    \"\"\"\n",
    "    pop_size = 5\n",
    "    tournament_size = 3\n",
    "    max_similar = 3\n",
    "    \"\"\"\n",
    "    #total_experiments = 5\n",
    "    count_experiments = 0\n",
    "    unroll = True\n",
    "\n",
    "    global_best_list = []\n",
    "    global_best = None\n",
    "    global_best_index = 0\n",
    "    experiment_times = np.zeros((total_experiments,1))\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "    t = datetime.datetime.now()\n",
    "\n",
    "    logging.basicConfig(filename='logs/nn_evolution_cmapss_' + t.strftime('%m%d%Y%H%M%S') + '.log', level=logging.INFO, \n",
    "                            format='%(levelname)s:%(threadName)s:%(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "    #cmaps datahandler\n",
    "    #dhandler_cmaps, input_shape = cmaps_dhandler()\n",
    "\n",
    "    \"\"\"\n",
    "    config = Configuration(architecture_type, problem_type, input_shape, output_shape, pop_size, tournament_size, \n",
    "                           max_similar, epochs=5, cross_val=0.2, size_scaler=size_scaler, max_generations=10, \n",
    "                           binary_selection=True, mutation_ratio=0.4, similarity_threshold=0.2, more_layers_prob=0.8)\n",
    "    \"\"\"\n",
    "\n",
    "    config.architecture_type = architecture_type\n",
    "    config.problem_type = problem_type\n",
    "    config.input_shape = input_shape\n",
    "    config.output_shape = output_shape\n",
    "    config.epochs=10\n",
    "\n",
    "    while count_experiments < total_experiments:\n",
    "        print(\"Launching experiment {}\".format(count_experiments+1))\n",
    "        logging.info(\"Launching experiment {}\".format(count_experiments+1))\n",
    "\n",
    "        start = time.time()\n",
    "        best = automatic_model_selection.run_experiment(config, dhandler_cmapss, count_experiments + 1, unroll=unroll,\n",
    "                                                        learningRate_scheduler=learningRate_scheduler, \n",
    "                                                        tModel_scaler=scaler)\n",
    "        end = time.time()\n",
    "        elapsed_time = (end-start)/60\n",
    "        experiment_times[count_experiments] = elapsed_time\n",
    "        print(\"Experiment time: {} minutes\".format(elapsed_time))\n",
    "        logging.info(\"Experiment time: {} minutes\".format(elapsed_time))\n",
    "\n",
    "        best.individual_label = count_experiments\n",
    "\n",
    "        global_best_list.append(best)\n",
    "\n",
    "        \"\"\"\n",
    "        if global_best == None:\n",
    "            global_best = best\n",
    "        else:\n",
    "            if best.fitness < global_best.fitness:\n",
    "                global_best = best\n",
    "                global_best_index = count_experiments\n",
    "        \"\"\"\n",
    "\n",
    "        count_experiments =  count_experiments + 1\n",
    "        \n",
    "    print(\"Recompute globals fitness\")\n",
    "    global_best_index = recompute_globals_fitness(global_best_list, config.size_scaler, config.problem_type)\n",
    "    global_best = global_best_list[global_best_index]\n",
    "        \n",
    "    total_experiment_time = experiment_times.sum()\n",
    "\n",
    "    print(\"Global best list\\n\")\n",
    "    logging.info(\"Global best list\\n\")\n",
    "    automatic_model_selection.print_best(global_best_list)\n",
    "\n",
    "    print(\"Global best is\\n\")\n",
    "    print(global_best)\n",
    "    logging.info(\"Global best is\\n\")\n",
    "    logging.info(global_best)\n",
    "\n",
    "    print(\"Global time {}\".format(total_experiment_time))\n",
    "    logging.info(\"Global time {}\".format(total_experiment_time))\n",
    "    \n",
    "    return global_best_list, global_best_index, total_experiment_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mnist_test(alphas):\n",
    "\n",
    "    experiments = 5\n",
    "    problem_type = 2\n",
    "\n",
    "    global_best_list = []\n",
    "    global_best_index = 0\n",
    "    total_experiment_time = []\n",
    "    total_experiment_time = 0\n",
    "    avg_experiment_time = 0\n",
    "\n",
    "    dHandler_mnist = MNISTDataHandler()\n",
    "\n",
    "    for size_scaler in alphas:\n",
    "\n",
    "        print(\"Running for alpha={}\".format(size_scaler))\n",
    "\n",
    "        global_best_list, global_best_index, total_experiment_time = mnist_test(dHandler_mnist, \n",
    "                                                                                size_scaler=size_scaler, \n",
    "                                                                                total_experiments=experiments)\n",
    "\n",
    "        avg_experiment_time = total_experiment_time/experiments\n",
    "\n",
    "        print(\"Total experiment time {}\".format(total_experiment_time))\n",
    "        print(\"Avg experiment time {}\".format(avg_experiment_time))\n",
    "\n",
    "        save_best_models(global_best_list, global_best_index, \n",
    "                         'best_models/mnist/yulin/alpha{}/'.format(size_scaler), input_shape=input_shape, \n",
    "                         data_handler=dHandler_mnist, problem_type=problem_type, train_epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cmapss_test(alphas):\n",
    "\n",
    "    experiments = 5\n",
    "    problem_type = 1\n",
    "\n",
    "    global_best_list = []\n",
    "    global_best_index = 0\n",
    "    total_experiment_time = []\n",
    "    total_experiment_time = 0\n",
    "    avg_experiment_time = 0\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "    dhandler_cmapss, input_shape = cmapss_dhandler()\n",
    "    dhandler_cmapss_for_best, input_shape = cmapss_dhandler()\n",
    "\n",
    "    for size_scaler in alphas:\n",
    "\n",
    "        print(\"Running for alpha={}\".format(size_scaler))\n",
    "\n",
    "        global_best_list, global_best_index, total_experiment_time = cmapss_test(dhandler_cmapss=dhandler_cmapss, \n",
    "                                                                                input_shape=input_shape,\n",
    "                                                                                size_scaler=size_scaler, \n",
    "                                                                                total_experiments=experiments)\n",
    "\n",
    "        print(global_best_list)\n",
    "        print(global_best_index)\n",
    "\n",
    "        avg_experiment_time = total_experiment_time/experiments\n",
    "\n",
    "        print(\"Total experiment time {}\".format(total_experiment_time))\n",
    "        print(\"Avg experiment time {}\".format(avg_experiment_time))\n",
    "\n",
    "        save_best_models(global_best_list, global_best_index, \n",
    "                         'best_models/cmapss/yulin/alpha{}/'.format(size_scaler), input_shape=input_shape, \n",
    "                         data_handler=dhandler_cmapss_for_best, problem_type=problem_type, train_epochs=100, \n",
    "                         data_scaler=scaler)\n",
    "        \n",
    "        return global_best_list, global_best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for alpha=0.7\n",
      "Launching experiment 1\n",
      "\n",
      "Generation 1\n",
      "launch new\n",
      "True\n",
      "gen similar\n",
      "False\n",
      "Fetching model 0 to keras\n",
      "Evaluating model 0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (Dense)                   (None, 520)               175240    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 432)               225072    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 433       \n",
      "=================================================================\n",
      "Total params: 400,745\n",
      "Trainable params: 400,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Loading data for the first time\n",
      "Reloading data due to parameter change\n",
      "Loading data for dataset 1 with window_size of 24, stride of 1 and maxRUL of 129. Cros-Validation ratio 0.2\n",
      "Loading data from file and computing dataframes\n",
      "training with cv\n",
      "Train on 14465 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      "14465/14465 [==============================] - 1s 44us/step - loss: 5151.2915 - mean_squared_error: 5151.2915 - val_loss: 5132.6973 - val_mean_squared_error: 5132.6973\n",
      "Epoch 2/10\n",
      "14465/14465 [==============================] - 0s 4us/step - loss: 3272.8887 - mean_squared_error: 3272.8887 - val_loss: 3899.5906 - val_mean_squared_error: 3899.5906\n",
      "Epoch 3/10\n",
      "14465/14465 [==============================] - 0s 4us/step - loss: 2387.8958 - mean_squared_error: 2387.8958 - val_loss: 3059.2251 - val_mean_squared_error: 3059.2251\n",
      "Epoch 4/10\n",
      "14465/14465 [==============================] - 0s 4us/step - loss: 1777.2962 - mean_squared_error: 1777.2962 - val_loss: 2463.5610 - val_mean_squared_error: 2463.5610\n",
      "Epoch 5/10\n",
      "14465/14465 [==============================] - 0s 4us/step - loss: 1338.4505 - mean_squared_error: 1338.4505 - val_loss: 1996.5094 - val_mean_squared_error: 1996.5094\n",
      "Epoch 6/10\n",
      "14465/14465 [==============================] - 0s 3us/step - loss: 998.1674 - mean_squared_error: 998.1674 - val_loss: 1616.3010 - val_mean_squared_error: 1616.3010\n",
      "Epoch 7/10\n",
      "14465/14465 [==============================] - 0s 3us/step - loss: 754.0147 - mean_squared_error: 754.0147 - val_loss: 1357.3827 - val_mean_squared_error: 1357.3827\n",
      "Epoch 8/10\n",
      "14465/14465 [==============================] - 0s 3us/step - loss: 590.0534 - mean_squared_error: 590.0534 - val_loss: 1176.9131 - val_mean_squared_error: 1176.9131\n",
      "Epoch 9/10\n",
      "14465/14465 [==============================] - 0s 3us/step - loss: 479.9189 - mean_squared_error: 479.9189 - val_loss: 1053.6016 - val_mean_squared_error: 1053.6016\n",
      "Epoch 10/10\n",
      "14465/14465 [==============================] - 0s 3us/step - loss: 403.6313 - mean_squared_error: 403.6313 - val_loss: 988.1688 - val_mean_squared_error: 988.1688\n",
      "20/20 [==============================] - 0s 22us/step\n",
      "Fetching model 1 to keras\n",
      "Evaluating model 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (Dense)                   (None, 1024)              345088    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 832)               852800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 176)               146608    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 176)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 936)               165672    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 936)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 184)               172408    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 185       \n",
      "=================================================================\n",
      "Total params: 1,682,761\n",
      "Trainable params: 1,682,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Using previously loaded data\n",
      "training with cv\n",
      "Train on 14465 samples, validate on 20 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1024,832] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training/Adam/mul_11}} = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Adam/beta_1/read, training/Adam/Variable_2/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss/mul/_141}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_899_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-677061736017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0malphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mglobal_best_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_best_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_cmapss_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-99c4b82d209c>\u001b[0m in \u001b[0;36mrun_cmapss_test\u001b[0;34m(alphas)\u001b[0m\n\u001b[1;32m     22\u001b[0m                                                                                 \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                                                 \u001b[0msize_scaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_scaler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                                                                                 total_experiments=experiments)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_best_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a2dc09410978>\u001b[0m in \u001b[0;36mcmapss_test\u001b[0;34m(dhandler_cmapss, input_shape, size_scaler, total_experiments)\u001b[0m\n\u001b[1;32m     49\u001b[0m         best = automatic_model_selection.run_experiment(config, dhandler_cmapss, count_experiments + 1, unroll=unroll,\n\u001b[1;32m     50\u001b[0m                                                         \u001b[0mlearningRate_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearningRate_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                                                         tModel_scaler=scaler)\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/controlslab/DATA/Projects/automatic_model_selection/code/automatic_model_selection.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(configuration, data_handler, experiment_number, unroll, learningRate_scheduler, tModel_scaler)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;31m#Assess the fitness of the inidividuals in the population\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \t\tbest_model, worst_model, worst_index = evaluate_population(population, configuration, data_handler, tModel_scaler,\n\u001b[0;32m--> 442\u001b[0;31m \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   unroll, learningRate_scheduler)\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                 \u001b[0;31m#Save worst and best models. Also append best model to elite archive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/controlslab/DATA/Projects/automatic_model_selection/code/automatic_model_selection.py\u001b[0m in \u001b[0;36mevaluate_population\u001b[0;34m(population, configuration, data_handler, tModel_scaler, unroll, learningRate_scheduler)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \t\tevaluate_individual(individual, configuration, data_handler, tModel_scaler, i, unroll,\n\u001b[0;32m--> 236\u001b[0;31m \t\t\t\t\t\t\tlearningRate_scheduler=learningRate_scheduler)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0mraw_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindividual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/controlslab/DATA/Projects/automatic_model_selection/code/automatic_model_selection.py\u001b[0m in \u001b[0;36mevaluate_individual\u001b[0;34m(individual, configuration, data_handler, tModel_scaler, ind_index, unroll, learningRate_scheduler)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \tindividual.compute_raw_scores(epochs=configuration.epochs, cross_validation_ratio=configuration.cross_val,\n\u001b[0;32m--> 213\u001b[0;31m \t\t\t\t      verbose=configuration.verbose_training, unroll=unroll, learningRate_scheduler=learningRate_scheduler)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mindividual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindividual_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mind_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/controlslab/DATA/Projects/automatic_model_selection/code/nn_evolutionary.py\u001b[0m in \u001b[0;36mcompute_raw_scores\u001b[0;34m(self, epochs, cross_validation_ratio, verbose, unroll, learningRate_scheduler)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainable_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_validation_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munroll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearningRate_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearningRate_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mmetric_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/controlslab/DATA/Projects/automatic_model_selection/code/nn_evolutionary.py\u001b[0m in \u001b[0;36mpartial_run\u001b[0;34m(self, cross_validation_ratio, epochs, verbose, unroll, learningRate_scheduler)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearningRate_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearningRate_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/controlslab/DATA/Projects/ann_framework/tunable_model/tunable_model.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, verbose, learningRate_scheduler, tf_session, tensorboard, get_minibatches_function_handle, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m                                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training with cv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                                 \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_X_crossVal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y_crossVal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training without cv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1024,832] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training/Adam/mul_11}} = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Adam/beta_1/read, training/Adam/Variable_2/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss/mul/_141}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_899_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "#alphas = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "alphas = [0.7]\n",
    "\n",
    "global_best_list, global_best_index = run_cmapss_test(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in global_best_list:\n",
    "    \n",
    "    print(ind)\n",
    "\n",
    "print(global_best_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alphas = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "#alphas = [0.8]\n",
    "\n",
    "#run_mnist_test(alphas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
