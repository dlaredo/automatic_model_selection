{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Model Selection\n",
    "\n",
    "Test notebook for automatic model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "#sys.path.append('/Users/davidlaredorazo/Documents/University_of_California/Research/Projects')\n",
    "sys.path.append('/media/controlslab/DATA/Projects')\n",
    "\n",
    "import ann_framework.aux_functions as aux_functions\n",
    "\n",
    "import automatic_model_selection\n",
    "from automatic_model_selection import Configuration\n",
    "from ann_encoding_rules import Layers\n",
    "import fetch_to_keras\n",
    "#from CMAPSAuxFunctions import TrainValTensorBoard\n",
    "\n",
    "#Tunable model\n",
    "from ann_framework.tunable_model.tunable_model import SequenceTunableModelRegression, SequenceTunableModelClassification\n",
    "\n",
    "#Data handlers\n",
    "from ann_framework.data_handlers.data_handler_CMAPSS import CMAPSSDataHandler\n",
    "from ann_framework.data_handlers.data_handler_MNIST import MNISTDataHandler\n",
    "from ann_framework.data_handlers.data_handler_CIFAR10 import CIFAR10DataHandler\n",
    "\n",
    "learningRate_scheduler = LearningRateScheduler(aux_functions.step_decay)\n",
    "\n",
    "size_scaler = 0.5\n",
    "\n",
    "#Use same configuration for all experiments, just change some of the parameters\n",
    "\n",
    "#Define some random paramaters for the creation of the configuration, this will change for each test model\n",
    "architecture_type = Layers.FullyConnected\n",
    "#architecture_type = Layers.Convolutional\n",
    "problem_type = 2  #1 for regression, 2 for classification\n",
    "output_shape = 10 #If regression applies, number of classes\n",
    "input_shape = (784,)\n",
    "\n",
    "config = Configuration(architecture_type, problem_type, input_shape, output_shape, pop_size=5, \n",
    "                       tournament_size=3, max_similar=3, epochs=5, cross_val=0.2, size_scaler=size_scaler,\n",
    "                       max_generations=1, binary_selection=True, mutation_ratio=0.8, \n",
    "                       similarity_threshold=0.2, more_layers_prob=0.4, verbose_individuals=True, \n",
    "                       show_model=True, verbose_training=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given a model get the compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model(model, problem_type, optimizer_params=[]):\n",
    "    \"\"\"Obtain a keras compiled model\"\"\"\n",
    "    \n",
    "    #Shared parameters for the models\n",
    "    optimizer = Adam(lr=0.01, beta_1=0.5)\n",
    "    \n",
    "    if problem_type == 1:\n",
    "        lossFunction = \"mean_squared_error\"\n",
    "        metrics = [\"mse\"]\n",
    "    elif problem_type == 2:\n",
    "        lossFunction = \"categorical_crossentropy\"\n",
    "        metrics = [\"accuracy\"]\n",
    "    else:\n",
    "        print(\"Problem type not defined\")\n",
    "        model = None\n",
    "        return\n",
    "    \n",
    "    #Create and compile the models\n",
    "    model.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_tunable_model(model_genotype, problem_type, input_shape, data_handler, model_number):\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    model = fetch_to_keras.decode_genotype(model_genotype, problem_type, input_shape, 1)\n",
    "    \n",
    "    model = get_compiled_model(model, problem_type, optimizer_params=[])\n",
    "    \n",
    "    if problem_type == 1:\n",
    "        tModel = SequenceTunableModelRegression('ModelReg_SN_'+str(model_number), model, lib_type='keras', data_handler=data_handler)\n",
    "    else:\n",
    "        tModel = SequenceTunableModelClassification('ModelClass_SN_'+str(model_number), model, lib_type='keras', data_handler=data_handler)\n",
    "        \n",
    "    return tModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the CMAPSS datahandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmapss_dhandler():\n",
    "\n",
    "    #Selected as per CNN paper\n",
    "    features = ['T2', 'T24', 'T30', 'T50', 'P2', 'P15', 'P30', 'Nf', 'Nc', 'epr', 'Ps30', 'phi', 'NRf', 'NRc', 'BPR', \n",
    "    'farB', 'htBleed', 'Nf_dmd', 'PCNfR_dmd', 'W31', 'W32']\n",
    "    selected_indices = np.array([2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21])\n",
    "    selected_features = list(features[i] for i in selected_indices-1)\n",
    "    data_folder = '../CMAPSSData'\n",
    "\n",
    "    window_size = 24\n",
    "    window_stride = 1\n",
    "    max_rul = 129\n",
    "\n",
    "    dhandler_cmapss = CMAPSSDataHandler(data_folder, 1, selected_features, max_rul, window_size, window_stride)\n",
    "\n",
    "    input_shape = (len(selected_features)*window_size, )\n",
    "\n",
    "    return dhandler_cmapss, input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to save top models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_models(best_models_list, global_best_model_index, saveto, input_shape, data_handler, \n",
    "                     problem_type=1, data_scaler=None, train_epochs=100, metrics=[], round=0):\n",
    "    \n",
    "    n_models = len(best_models_list)\n",
    "    \n",
    "    for ind_model, i in zip(best_models_list, range(n_models)):\n",
    "        \n",
    "        tModel = create_tunable_model(ind_model.stringModel, problem_type, input_shape, data_handler, i)\n",
    "        kmodel = tModel.model\n",
    "        model_json = kmodel.to_json()\n",
    "        \n",
    "        #Save model's architecture\n",
    "        string_append = str(i) if i != global_best_model_index else 'global'\n",
    "        with open(saveto+\"bestModel_\"+string_append+\".json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "            \n",
    "    #Train the global best, model has to be recompiled\n",
    "    ind_model = best_models_list[global_best_model_index]\n",
    "    tModel = create_tunable_model(ind_model.stringModel, problem_type, input_shape, data_handler, n_models)\n",
    "    \n",
    "    print(tModel.model.summary())\n",
    "    print(tModel.data_handler)\n",
    "    \n",
    "    if tModel.data_handler.data_scaler != None:\n",
    "        print(\"Using data handler scaler\")\n",
    "    elif tModel.data_scaler != None:\n",
    "        print(\"Using tModel scaler (Overriding data handler scaler)\")\n",
    "    else:\n",
    "        print(\"No data scaling used\")\n",
    "    \n",
    "    if data_scaler != None:\n",
    "        tModel.data_handler.data_scaler = None\n",
    "        tModel.data_scaler = data_scaler\n",
    "        \n",
    "    tModel.load_data(unroll=True, verbose=1, cross_validation_ratio=0)\n",
    "    tModel.print_data()\n",
    "    tModel.epochs = train_epochs\n",
    "\n",
    "    tModel.train_model(verbose=1)\n",
    "    \n",
    "    tModel.evaluate_model(metrics, round=round)\n",
    "    \n",
    "    kmodel = tModel.model\n",
    "            \n",
    "    # serialize weights to HDF5\n",
    "    kmodel.save_weights(saveto+\"bestModel_global.h5\")\n",
    "    \n",
    "    print(\"Saved models for dataset 1 to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get global best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_globals_fitness(best_models, size_scaler, problem_type):\n",
    "    \"\"\"It is necessary to recompute the fiteness of global models since they have differnt normalization factors\"\"\"\n",
    "\n",
    "    #print(\"Before normalization\")\n",
    "    #automatic_model_selection.print_best(best_models)\n",
    "    \n",
    "    normalize_scores(best_models)\n",
    "    \n",
    "    #print(\"After normalization\")\n",
    "    #automatic_model_selection.print_best(best_models)\n",
    "    \n",
    "    global_best_index = compute_fitness(best_models, size_scaler, problem_type)\n",
    "    \n",
    "    print(\"Recomputed fitness\")\n",
    "    automatic_model_selection.print_best(best_models)\n",
    "    print(\"Global best index\")\n",
    "    print(global_best_index)\n",
    "    \n",
    "    return global_best_index\n",
    "\n",
    "\n",
    "def normalize_scores(best_models):\n",
    "    \n",
    "    pop_size = len(best_models)\n",
    "    raw_scores = np.zeros((pop_size,))\n",
    "    \n",
    "    for i in range(pop_size):\n",
    "        model = best_models[i]\n",
    "        raw_scores[i] = model.raw_score\n",
    "        \n",
    "    normalization_factor = np.linalg.norm(raw_scores)\n",
    "    normalized_scores = raw_scores/normalization_factor\n",
    "    \n",
    "    for i in range(pop_size):\n",
    "        model = best_models[i]\n",
    "        model.normalized_score = raw_scores[i]\n",
    "    \n",
    "    \n",
    "def compute_fitness(best_models, size_scaler, problem_type):\n",
    "    \n",
    "    pop_size = len(best_models)\n",
    "    \n",
    "    global_best_index = 0\n",
    "    \n",
    "    for i in range(pop_size):\n",
    "        \n",
    "        round_up_to = 3\n",
    "\n",
    "        #Round up to the first 3 digits before computing log                                                                                                                                                          \n",
    "        rounding_scaler = 10**round_up_to\n",
    "        trainable_count = round(best_models[i].raw_size/rounding_scaler)*rounding_scaler\n",
    "        size_score = math.log10(trainable_count)\n",
    "\n",
    "        scaled_score = best_models[i].normalized_score\n",
    "\n",
    "        #For classification estimate the error which is between 0 and 1                                                                                                                   \n",
    "        if problem_type == 2:\n",
    "            metric_score = (1 - scaled_score)*10 #Multiply by 10 to have a better scaling. I still need to find an appropriate scaling                                                \n",
    "        else:\n",
    "            metric_score = scaled_score*10 #Multiply by 10 to have a better scaling. I still need to find an appropiate scaling                                                       \n",
    "    \n",
    "        metric_scaler = 1-size_scaler\n",
    "        print(\"metric_scaler %f\"%metric_scaler)\n",
    "        print(\"size scaler %f\"%size_scaler)\n",
    "    \n",
    "        #Scalarization of multiobjective version of the fitness function                                                                                                                  \n",
    "        best_models[i].fitness = metric_scaler*metric_score + size_scaler*size_score\n",
    "        \n",
    "        if best_models[i].fitness < best_models[global_best_index].fitness:\n",
    "            global_best_index = i\n",
    "            \n",
    "    return global_best_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_test(dhandler_mnist, size_scaler=0.5, total_experiments=3):\n",
    "\n",
    "    \"\"\"Input can be of 3 types, ANN (1), CNN (2) or RNN (3)\"\"\"\n",
    "    architecture_type = Layers.FullyConnected\n",
    "    #architecture_type = Layers.Convolutional\n",
    "    problem_type = 2  #1 for regression, 2 for classification\n",
    "    output_shape = 10 #If regression applies, number of classes\n",
    "    input_shape = (784,)\n",
    "    #input_shape = (28,28,1)\n",
    "\n",
    "    \"\"\"\n",
    "    pop_size = 5\n",
    "    tournament_size = 3\n",
    "    max_similar = 3\n",
    "    total_experiments = 5\n",
    "    count_experiments = 0\n",
    "    unroll = True\n",
    "    \"\"\"\n",
    "    #total_experiments = 1\n",
    "    count_experiments = 0\n",
    "    unroll = True\n",
    "\n",
    "    global_best_list = []\n",
    "    global_best = None\n",
    "    global_best_index = 0\n",
    "    experiment_times = np.zeros((total_experiments,1))\n",
    "\n",
    "    scaler = None\n",
    "\n",
    "    t = datetime.datetime.now()\n",
    "    \n",
    "    #Clear logging facility before attempting to create log\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    \n",
    "    logging.basicConfig(filename='logs/nn_evolution_mnist_' + t.strftime('%m%d%Y%H%M%S') + '.log', level=logging.INFO, \n",
    "                            format='%(levelname)s:%(threadName)s:%(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "    #mnist datahandler\n",
    "    #dHandler_mnist = MNISTDataHandler()\n",
    "\n",
    "    config.architecture_type = architecture_type\n",
    "    config.problem_type = problem_type\n",
    "    config.input_shape = input_shape\n",
    "    config.output_shape = output_shape\n",
    "    config.size_scaler = size_scaler\n",
    "\n",
    "    \"\"\"\n",
    "    config = Configuration(architecture_type, problem_type, input_shape, output_shape, pop_size, \n",
    "                           tournament_size, max_similar, epochs=20, cross_val=0.2, size_scaler=size_scaler,\n",
    "                           max_generations=10, binary_selection=True, mutation_ratio=0.4, \n",
    "                           similarity_threshold=0.2, more_layers_prob=0.8)\n",
    "    \"\"\"\n",
    "\n",
    "    while count_experiments < total_experiments:\n",
    "        print(\"Launching experiment {}\".format(count_experiments+1))\n",
    "        logging.info(\"Launching experiment {}\".format(count_experiments+1))\n",
    "\n",
    "\n",
    "        start = time.time()\n",
    "        best = automatic_model_selection.run_experiment(config, dHandler_mnist, count_experiments + 1, unroll=unroll,\n",
    "                                                        learningRate_scheduler=learningRate_scheduler, \n",
    "                                                        tModel_scaler=scaler)\n",
    "        end = time.time()\n",
    "        elapsed_time = (end-start)/60\n",
    "        experiment_times[count_experiments] = elapsed_time\n",
    "        print(\"Experiment time: {} minutes\".format(elapsed_time))\n",
    "        logging.info(\"Experiment time: {} minutes\".format(elapsed_time))\n",
    "\n",
    "\n",
    "        best.individual_label = count_experiments\n",
    "\n",
    "        global_best_list.append(best)\n",
    "        \n",
    "        \"\"\"\n",
    "        if global_best == None:\n",
    "            global_best = best\n",
    "        else:\n",
    "            if best.fitness < global_best.fitness:\n",
    "                global_best = best\n",
    "                global_best_index = count_experiments\n",
    "        \"\"\"\n",
    "\n",
    "        count_experiments =  count_experiments + 1\n",
    "    \n",
    "    print(\"Recompute globals fitness\")\n",
    "    global_best_index = recompute_globals_fitness(global_best_list, config.size_scaler, config.problem_type)\n",
    "    global_best = global_best_list[global_best_index]\n",
    "    \n",
    "    total_experiment_time = experiment_times.sum()\n",
    "\n",
    "    print(\"Global best list\\n\")\n",
    "    logging.info(\"Global best list\\n\")\n",
    "    automatic_model_selection.print_best(global_best_list)\n",
    "\n",
    "    print(\"Global best is\\n\")\n",
    "    print(global_best)\n",
    "    logging.info(\"Global best is\\n\")\n",
    "    logging.info(global_best)\n",
    "\n",
    "    print(\"Global time {}\".format(experiment_times.sum()))\n",
    "    logging.info(\"Global time {}\".format(experiment_times.sum()))\n",
    "    \n",
    "    logging.shutdown()\n",
    "    \n",
    "    return global_best_list, global_best_index, total_experiment_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_test(dHandler_cifar, size_scaler=0.5, total_experiments=3):\n",
    "\n",
    "    \"\"\"Input can be of 3 types, ANN (1), CNN (2) or RNN (3)\"\"\"\n",
    "    architecture_type = Layers.FullyConnected\n",
    "    problem_type = 2  #1 for regression, 2 for classification\n",
    "    output_shape = 10 #If regression applies, number of classes\n",
    "    input_shape = (3072,)\n",
    "    \"\"\"\n",
    "    pop_size = 5\n",
    "    tournament_size = 3\n",
    "    max_similar = 3\n",
    "    \"\"\"\n",
    "    total_experiments = 5\n",
    "    count_experiments = 0\n",
    "    unroll = True\n",
    "\n",
    "    global_best_list = []\n",
    "    global_best = None\n",
    "    global_best_index = 0\n",
    "\n",
    "    scaler = None\n",
    "\n",
    "    t = datetime.datetime.now()\n",
    "\n",
    "    logging.basicConfig(filename='logs/nn_evolution_cifar10_' + t.strftime('%m%d%Y%H%M%S') + '.log', level=logging.INFO, \n",
    "                            format='%(levelname)s:%(threadName)s:%(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "    #mnist datahandler\n",
    "    #dHandler_cifar = CIFAR10DataHandler()\n",
    "\n",
    "    \"\"\"\n",
    "    config = Configuration(architecture_type, problem_type, input_shape, output_shape, pop_size, tournament_size, max_similar, \n",
    "                           epochs=5, cross_val=0.2, size_scaler=size_scaler, max_generations=10, binary_selection=True, \n",
    "                           mutation_ratio=0.4, similarity_threshold=0.2, more_layers_prob=0.8)\n",
    "    \"\"\"\n",
    "\n",
    "    config.architecture_type = architecture_type\n",
    "    config.problem_type = problem_type\n",
    "    config.input_shape = input_shape\n",
    "    config.output_shape = output_shape\n",
    "\n",
    "    while count_experiments < total_experiments:\n",
    "        print(\"Launching experiment {}\".format(count_experiments+1))\n",
    "        logging.info(\"Launching experiment {}\".format(count_experiments+1))\n",
    "\n",
    "        best = automatic_model_selection.run_experiment(config, dHandler_cifar, count_experiments + 1, unroll=unroll,\n",
    "                                                        learningRate_scheduler=learningRate_scheduler, \n",
    "                                                        tModel_scaler=scaler, verbose_data=0)\n",
    "\n",
    "        best.individual_label = count_experiments\n",
    "\n",
    "        global_best_list.append(best)\n",
    "\n",
    "        if global_best == None:\n",
    "            global_best = best\n",
    "        else:\n",
    "            if best.fitness < global_best.fitness:\n",
    "                global_best = best\n",
    "                global_best_index = count_experiments\n",
    "\n",
    "        count_experiments =  count_experiments + 1\n",
    "        \n",
    "    total_experiment_time = experiment_times.sum()\n",
    "\n",
    "    print(\"Global best list\\n\")\n",
    "    logging.info(\"Global best list\\n\")\n",
    "    automatic_model_selection.print_best(global_best_list)\n",
    "\n",
    "    print(\"Global best is\\n\")\n",
    "    print(global_best)\n",
    "    logging.info(\"Global best is\\n\")\n",
    "    logging.info(global_best)\n",
    "    \n",
    "    return global_best_list, global_best_index, total_experiment_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on CMAPSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmapss_test(dhandler_cmapss, input_shape, size_scaler=0.5, total_experiments=3):\n",
    "\n",
    "    \"\"\"Input can be of 3 types, ANN (1), CNN (2) or RNN (3)\"\"\"\n",
    "    architecture_type = Layers.FullyConnected\n",
    "    problem_type = 1  #1 for regression, 2 for classification\n",
    "    output_shape = 1 #If regression applies, number of classes\n",
    "\n",
    "    \"\"\"\n",
    "    pop_size = 5\n",
    "    tournament_size = 3\n",
    "    max_similar = 3\n",
    "    \"\"\"\n",
    "    #total_experiments = 5\n",
    "    count_experiments = 0\n",
    "    unroll = True\n",
    "\n",
    "    global_best_list = []\n",
    "    global_best = None\n",
    "    global_best_index = 0\n",
    "    experiment_times = np.zeros((total_experiments,1))\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "    t = datetime.datetime.now()\n",
    "\n",
    "    logging.basicConfig(filename='logs/nn_evolution_cmapss_' + t.strftime('%m%d%Y%H%M%S') + '.log', level=logging.INFO, \n",
    "                            format='%(levelname)s:%(threadName)s:%(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "    #cmaps datahandler\n",
    "    #dhandler_cmaps, input_shape = cmaps_dhandler()\n",
    "\n",
    "    \"\"\"\n",
    "    config = Configuration(architecture_type, problem_type, input_shape, output_shape, pop_size, tournament_size, \n",
    "                           max_similar, epochs=5, cross_val=0.2, size_scaler=size_scaler, max_generations=10, \n",
    "                           binary_selection=True, mutation_ratio=0.4, similarity_threshold=0.2, more_layers_prob=0.8)\n",
    "    \"\"\"\n",
    "\n",
    "    config.architecture_type = architecture_type\n",
    "    config.problem_type = problem_type\n",
    "    config.input_shape = input_shape\n",
    "    config.output_shape = output_shape\n",
    "\n",
    "    while count_experiments < total_experiments:\n",
    "        print(\"Launching experiment {}\".format(count_experiments+1))\n",
    "        logging.info(\"Launching experiment {}\".format(count_experiments+1))\n",
    "\n",
    "        start = time.time()\n",
    "        best = automatic_model_selection.run_experiment(config, dhandler_cmapss, count_experiments + 1, unroll=unroll,\n",
    "                                                        learningRate_scheduler=learningRate_scheduler, \n",
    "                                                        tModel_scaler=scaler)\n",
    "        end = time.time()\n",
    "        elapsed_time = (end-start)/60\n",
    "        experiment_times[count_experiments] = elapsed_time\n",
    "        print(\"Experiment time: {} minutes\".format(elapsed_time))\n",
    "        logging.info(\"Experiment time: {} minutes\".format(elapsed_time))\n",
    "\n",
    "        best.individual_label = count_experiments\n",
    "\n",
    "        global_best_list.append(best)\n",
    "\n",
    "        \"\"\"\n",
    "        if global_best == None:\n",
    "            global_best = best\n",
    "        else:\n",
    "            if best.fitness < global_best.fitness:\n",
    "                global_best = best\n",
    "                global_best_index = count_experiments\n",
    "        \"\"\"\n",
    "\n",
    "        count_experiments =  count_experiments + 1\n",
    "        \n",
    "    print(\"Recompute globals fitness\")\n",
    "    global_best_index = recompute_globals_fitness(global_best_list, config.size_scaler, config.problem_type)\n",
    "    global_best = global_best_list[global_best_index]\n",
    "        \n",
    "    total_experiment_time = experiment_times.sum()\n",
    "\n",
    "    print(\"Global best list\\n\")\n",
    "    logging.info(\"Global best list\\n\")\n",
    "    automatic_model_selection.print_best(global_best_list)\n",
    "\n",
    "    print(\"Global best is\\n\")\n",
    "    print(global_best)\n",
    "    logging.info(\"Global best is\\n\")\n",
    "    logging.info(global_best)\n",
    "\n",
    "    print(\"Global time {}\".format(total_experiment_time))\n",
    "    logging.info(\"Global time {}\".format(total_experiment_time))\n",
    "    \n",
    "    return global_best_list, global_best_index, total_experiment_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n#alphas = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\\nalphas = [0.5]\\nexperiments = 5\\n\\nglobal_best_list = []\\nglobal_best_index = 0\\ntotal_experiment_time = []\\ntotal_experiment_time = 0\\navg_experiment_time = 0\\n\\ndHandler_mnist = MNISTDataHandler()\\n\\nfor size_scaler in alphas:\\n\\n    print(\"Running for alpha={}\".format(size_scaler))\\n    \\n    global_best_list, global_best_index, total_experiment_time = mnist_test(dHandler_mnist, \\n                                                                            size_scaler=size_scaler, \\n                                                                            total_experiments=experiments)\\n    \\n    avg_experiment_time = total_experiment_time/experiments\\n    \\n    print(\"Total experiment time {}\".format(total_experiment_time))\\n    print(\"Avg experiment time {}\".format(avg_experiment_time))\\n    \\n    save_best_models(global_best_list, global_best_index, \\n                     \\'best_models/mnist/yulin/alpha{}/\\'.format(size_scaler), input_shape=input_shape, \\n                     data_handler=dHandler_mnist, problem_type=problem_type, train_epochs=100)\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "#alphas = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "alphas = [0.5]\n",
    "experiments = 5\n",
    "\n",
    "global_best_list = []\n",
    "global_best_index = 0\n",
    "total_experiment_time = []\n",
    "total_experiment_time = 0\n",
    "avg_experiment_time = 0\n",
    "\n",
    "dHandler_mnist = MNISTDataHandler()\n",
    "\n",
    "for size_scaler in alphas:\n",
    "\n",
    "    print(\"Running for alpha={}\".format(size_scaler))\n",
    "    \n",
    "    global_best_list, global_best_index, total_experiment_time = mnist_test(dHandler_mnist, \n",
    "                                                                            size_scaler=size_scaler, \n",
    "                                                                            total_experiments=experiments)\n",
    "    \n",
    "    avg_experiment_time = total_experiment_time/experiments\n",
    "    \n",
    "    print(\"Total experiment time {}\".format(total_experiment_time))\n",
    "    print(\"Avg experiment time {}\".format(avg_experiment_time))\n",
    "    \n",
    "    save_best_models(global_best_list, global_best_index, \n",
    "                     'best_models/mnist/yulin/alpha{}/'.format(size_scaler), input_shape=input_shape, \n",
    "                     data_handler=dHandler_mnist, problem_type=problem_type, train_epochs=100)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\ndHandler_cifar = CIFAR10DataHandler()\\n\\nglobal_best_list, global_best_index = cifar_test(dHandler_cifar, size_scaler=size_scaler, total_experiments=1)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "dHandler_cifar = CIFAR10DataHandler()\n",
    "\n",
    "global_best_list, global_best_index = cifar_test(dHandler_cifar, size_scaler=size_scaler, total_experiments=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el problema es\n",
      "1\n",
      "Running for alpha=0.5\n",
      "Launching experiment 1\n",
      "\n",
      "Generation 1\n",
      "launch new\n",
      "True\n",
      "gen similar\n",
      "False\n",
      "Fetching model 0 to keras\n",
      "Creating singleton instance\n",
      "Evaluating model 0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (Dense)                   (None, 104)               35048     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 104)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 288)               30240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 289       \n",
      "=================================================================\n",
      "Total params: 65,577\n",
      "Trainable params: 65,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Loading data for the first time\n",
      "Reloading data due to parameter change\n",
      "Loading data for dataset 1 with window_size of 24, stride of 1 and maxRUL of 129. Cros-Validation ratio 0.2\n",
      "Loading data from file and computing dataframes\n",
      "printing crossval\n",
      "[[ 642.82   1592.39   1411.94   ...  393.       38.57     23.0357]\n",
      " [ 642.33   1587.05   1409.73   ...  394.       38.9      23.3115]\n",
      " [ 642.73   1591.61   1403.71   ...  393.       38.81     23.2616]\n",
      " ...\n",
      " [ 642.21   1582.66   1408.32   ...  394.       38.78     23.2137]\n",
      " [ 642.9    1589.83   1411.58   ...  393.       38.82     23.2623]\n",
      " [ 642.2    1590.2    1399.28   ...  393.       38.79     23.2749]]\n",
      "training with cv\n",
      "Train on 14927 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      "14927/14927 [==============================] - 0s 15us/step - loss: 6715.7848 - mean_squared_error: 6715.7848 - val_loss: 4933.7295 - val_mean_squared_error: 4933.7295\n",
      "Epoch 2/5\n",
      "14927/14927 [==============================] - 0s 5us/step - loss: 4396.1943 - mean_squared_error: 4396.1943 - val_loss: 4053.6848 - val_mean_squared_error: 4053.6848\n",
      "Epoch 3/5\n",
      "14927/14927 [==============================] - 0s 5us/step - loss: 3462.9921 - mean_squared_error: 3462.9921 - val_loss: 3376.1724 - val_mean_squared_error: 3376.1724\n",
      "Epoch 4/5\n",
      "14927/14927 [==============================] - 0s 5us/step - loss: 2784.7172 - mean_squared_error: 2784.7172 - val_loss: 2838.7354 - val_mean_squared_error: 2838.7354\n",
      "Epoch 5/5\n",
      "14927/14927 [==============================] - 0s 5us/step - loss: 2265.0164 - mean_squared_error: 2265.0164 - val_loss: 2426.2207 - val_mean_squared_error: 2426.2207\n",
      "20/20 [==============================] - 0s 14us/step\n",
      "Fetching model 1 to keras\n",
      "Reusing singleton instance\n",
      "<ann_framework.data_handlers.data_handler_CMAPSS.CMAPSSDataHandler object at 0x7f6887648550>\n",
      "[[-0.54362416 -0.34526854 -0.6456044  ... -0.6         0.6124031\n",
      "   0.41505195]\n",
      " [-0.59060403 -0.18107417 -0.57299843 ... -0.6         0.31782946\n",
      "   0.66835159]\n",
      " [-0.77181208 -0.11611253 -0.34419152 ... -0.6         0.27131783\n",
      "   0.61443415]\n",
      " ...\n",
      " [ 0.10067114  0.58516624  0.33398744 ...  0.8        -0.53488372\n",
      "  -0.89019938]\n",
      " [ 0.42281879  0.55498721  0.53649922 ...  0.2        -0.76744186\n",
      "  -0.52316765]\n",
      " [ 0.4295302   0.13452685  0.43877551 ...  0.4        -0.64341085\n",
      "  -0.55630441]]\n",
      "[[-0.10738255 -0.49616368 -0.26844584 ... -0.2         0.03875969\n",
      "   0.29458017]\n",
      " [-0.12751678 -0.18158568  0.02197802 ... -0.6         0.03875969\n",
      "   0.0322943 ]\n",
      " [ 0.06711409 -0.05473146  0.13814757 ...  0.2         0.2248062\n",
      "   0.06655434]\n",
      " ...\n",
      " [-0.10738255 -0.18925831 -0.07417582 ...  0.         -0.03875969\n",
      "   0.31030609]\n",
      " [-0.60402685 -0.53913043 -0.40816327 ... -0.6         0.25581395\n",
      "   0.30665543]\n",
      " [-0.30201342  0.22966752 -0.36381476 ...  0.4        -0.13178295\n",
      "  -0.18197136]]\n",
      "[[ 0.08053691  0.09207161  0.16522763 ... -0.2        -0.33333333\n",
      "  -0.60263971]\n",
      " [-0.24832215 -0.18107417  0.07849294 ...  0.          0.17829457\n",
      "   0.17186184]\n",
      " [ 0.02013423  0.05217391 -0.1577708  ... -0.2         0.03875969\n",
      "   0.03173266]\n",
      " ...\n",
      " [-0.32885906 -0.4056266   0.02315542 ...  0.         -0.00775194\n",
      "  -0.10278012]\n",
      " [ 0.13422819 -0.03887468  0.1510989  ... -0.2         0.05426357\n",
      "   0.0336984 ]\n",
      " [-0.33557047 -0.01994885 -0.33163265 ... -0.2         0.00775194\n",
      "   0.06908172]]\n",
      "Evaluating model 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (Dense)                   (None, 208)               70096     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 208)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 160)               33440     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 103,697\n",
      "Trainable params: 103,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Using previously loaded data\n",
      "printing crossval\n",
      "[[ 642.82   1592.39   1411.94   ...  393.       38.57     23.0357]\n",
      " [ 642.33   1587.05   1409.73   ...  394.       38.9      23.3115]\n",
      " [ 642.73   1591.61   1403.71   ...  393.       38.81     23.2616]\n",
      " ...\n",
      " [ 642.21   1582.66   1408.32   ...  394.       38.78     23.2137]\n",
      " [ 642.9    1589.83   1411.58   ...  393.       38.82     23.2623]\n",
      " [ 642.2    1590.2    1399.28   ...  393.       38.79     23.2749]]\n",
      "training with cv\n",
      "Train on 14927 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      "14927/14927 [==============================] - 0s 15us/step - loss: 8151.8260 - mean_squared_error: 8151.8260 - val_loss: 7047.2764 - val_mean_squared_error: 7047.2764\n",
      "Epoch 2/5\n",
      "14927/14927 [==============================] - 0s 6us/step - loss: 7352.2732 - mean_squared_error: 7352.2732 - val_loss: 6645.6064 - val_mean_squared_error: 6645.6064\n",
      "Epoch 3/5\n",
      "14927/14927 [==============================] - 0s 6us/step - loss: 6903.4909 - mean_squared_error: 6903.4909 - val_loss: 6307.3389 - val_mean_squared_error: 6307.3389\n",
      "Epoch 4/5\n",
      "14927/14927 [==============================] - 0s 6us/step - loss: 6506.9659 - mean_squared_error: 6506.9659 - val_loss: 6007.3408 - val_mean_squared_error: 6007.3408\n",
      "Epoch 5/5\n",
      "14927/14927 [==============================] - 0s 6us/step - loss: 6142.6309 - mean_squared_error: 6142.6309 - val_loss: 5720.8926 - val_mean_squared_error: 5720.8926\n",
      "20/20 [==============================] - 0s 16us/step\n",
      "Fetching model 2 to keras\n",
      "Reusing singleton instance\n",
      "<ann_framework.data_handlers.data_handler_CMAPSS.CMAPSSDataHandler object at 0x7f6887648550>\n",
      "[[-0.54362416 -0.34526854 -0.6456044  ... -0.6         0.6124031\n",
      "   0.41505195]\n",
      " [-0.59060403 -0.18107417 -0.57299843 ... -0.6         0.31782946\n",
      "   0.66835159]\n",
      " [-0.77181208 -0.11611253 -0.34419152 ... -0.6         0.27131783\n",
      "   0.61443415]\n",
      " ...\n",
      " [ 0.10067114  0.58516624  0.33398744 ...  0.8        -0.53488372\n",
      "  -0.89019938]\n",
      " [ 0.42281879  0.55498721  0.53649922 ...  0.2        -0.76744186\n",
      "  -0.52316765]\n",
      " [ 0.4295302   0.13452685  0.43877551 ...  0.4        -0.64341085\n",
      "  -0.55630441]]\n",
      "[[-0.10738255 -0.49616368 -0.26844584 ... -0.2         0.03875969\n",
      "   0.29458017]\n",
      " [-0.12751678 -0.18158568  0.02197802 ... -0.6         0.03875969\n",
      "   0.0322943 ]\n",
      " [ 0.06711409 -0.05473146  0.13814757 ...  0.2         0.2248062\n",
      "   0.06655434]\n",
      " ...\n",
      " [-0.10738255 -0.18925831 -0.07417582 ...  0.         -0.03875969\n",
      "   0.31030609]\n",
      " [-0.60402685 -0.53913043 -0.40816327 ... -0.6         0.25581395\n",
      "   0.30665543]\n",
      " [-0.30201342  0.22966752 -0.36381476 ...  0.4        -0.13178295\n",
      "  -0.18197136]]\n",
      "[[ 0.08053691  0.09207161  0.16522763 ... -0.2        -0.33333333\n",
      "  -0.60263971]\n",
      " [-0.24832215 -0.18107417  0.07849294 ...  0.          0.17829457\n",
      "   0.17186184]\n",
      " [ 0.02013423  0.05217391 -0.1577708  ... -0.2         0.03875969\n",
      "   0.03173266]\n",
      " ...\n",
      " [-0.32885906 -0.4056266   0.02315542 ...  0.         -0.00775194\n",
      "  -0.10278012]\n",
      " [ 0.13422819 -0.03887468  0.1510989  ... -0.2         0.05426357\n",
      "   0.0336984 ]\n",
      " [-0.33557047 -0.01994885 -0.33163265 ... -0.2         0.00775194\n",
      "   0.06908172]]\n",
      "Evaluating model 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (Dense)                   (None, 696)               234552    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 696)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 856)               596632    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 856)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 768)               658176    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 168)               129192    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 169       \n",
      "=================================================================\n",
      "Total params: 1,618,721\n",
      "Trainable params: 1,618,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Using previously loaded data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing crossval\n",
      "[[ 642.82   1592.39   1411.94   ...  393.       38.57     23.0357]\n",
      " [ 642.33   1587.05   1409.73   ...  394.       38.9      23.3115]\n",
      " [ 642.73   1591.61   1403.71   ...  393.       38.81     23.2616]\n",
      " ...\n",
      " [ 642.21   1582.66   1408.32   ...  394.       38.78     23.2137]\n",
      " [ 642.9    1589.83   1411.58   ...  393.       38.82     23.2623]\n",
      " [ 642.2    1590.2    1399.28   ...  393.       38.79     23.2749]]\n",
      "training with cv\n",
      "Train on 14927 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      "14927/14927 [==============================] - 1s 79us/step - loss: 6404.9338 - mean_squared_error: 6404.9338 - val_loss: 5658.0400 - val_mean_squared_error: 5658.0400\n",
      "Epoch 2/5\n",
      "14927/14927 [==============================] - 1s 62us/step - loss: 5569.7123 - mean_squared_error: 5569.7123 - val_loss: 5130.3779 - val_mean_squared_error: 5130.3779\n",
      "Epoch 3/5\n",
      "14927/14927 [==============================] - 1s 62us/step - loss: 4971.6185 - mean_squared_error: 4971.6185 - val_loss: 4679.8984 - val_mean_squared_error: 4679.8984\n",
      "Epoch 4/5\n",
      "14927/14927 [==============================] - 1s 61us/step - loss: 4446.5686 - mean_squared_error: 4446.5686 - val_loss: 4261.0811 - val_mean_squared_error: 4261.0811\n",
      "Epoch 5/5\n",
      "14927/14927 [==============================] - 1s 61us/step - loss: 3979.0650 - mean_squared_error: 3979.0650 - val_loss: 3897.2039 - val_mean_squared_error: 3897.2039\n",
      "20/20 [==============================] - 0s 63us/step\n",
      "Fetching model 3 to keras\n",
      "Reusing singleton instance\n",
      "<ann_framework.data_handlers.data_handler_CMAPSS.CMAPSSDataHandler object at 0x7f6887648550>\n",
      "[[-0.54362416 -0.34526854 -0.6456044  ... -0.6         0.6124031\n",
      "   0.41505195]\n",
      " [-0.59060403 -0.18107417 -0.57299843 ... -0.6         0.31782946\n",
      "   0.66835159]\n",
      " [-0.77181208 -0.11611253 -0.34419152 ... -0.6         0.27131783\n",
      "   0.61443415]\n",
      " ...\n",
      " [ 0.10067114  0.58516624  0.33398744 ...  0.8        -0.53488372\n",
      "  -0.89019938]\n",
      " [ 0.42281879  0.55498721  0.53649922 ...  0.2        -0.76744186\n",
      "  -0.52316765]\n",
      " [ 0.4295302   0.13452685  0.43877551 ...  0.4        -0.64341085\n",
      "  -0.55630441]]\n",
      "[[-0.10738255 -0.49616368 -0.26844584 ... -0.2         0.03875969\n",
      "   0.29458017]\n",
      " [-0.12751678 -0.18158568  0.02197802 ... -0.6         0.03875969\n",
      "   0.0322943 ]\n",
      " [ 0.06711409 -0.05473146  0.13814757 ...  0.2         0.2248062\n",
      "   0.06655434]\n",
      " ...\n",
      " [-0.10738255 -0.18925831 -0.07417582 ...  0.         -0.03875969\n",
      "   0.31030609]\n",
      " [-0.60402685 -0.53913043 -0.40816327 ... -0.6         0.25581395\n",
      "   0.30665543]\n",
      " [-0.30201342  0.22966752 -0.36381476 ...  0.4        -0.13178295\n",
      "  -0.18197136]]\n",
      "[[ 0.08053691  0.09207161  0.16522763 ... -0.2        -0.33333333\n",
      "  -0.60263971]\n",
      " [-0.24832215 -0.18107417  0.07849294 ...  0.          0.17829457\n",
      "   0.17186184]\n",
      " [ 0.02013423  0.05217391 -0.1577708  ... -0.2         0.03875969\n",
      "   0.03173266]\n",
      " ...\n",
      " [-0.32885906 -0.4056266   0.02315542 ...  0.         -0.00775194\n",
      "  -0.10278012]\n",
      " [ 0.13422819 -0.03887468  0.1510989  ... -0.2         0.05426357\n",
      "   0.0336984 ]\n",
      " [-0.33557047 -0.01994885 -0.33163265 ... -0.2         0.00775194\n",
      "   0.06908172]]\n",
      "Evaluating model 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (Dense)                   (None, 880)               296560    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 880)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 760)               669560    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 761       \n",
      "=================================================================\n",
      "Total params: 966,881\n",
      "Trainable params: 966,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Using previously loaded data\n",
      "printing crossval\n",
      "[[ 642.82   1592.39   1411.94   ...  393.       38.57     23.0357]\n",
      " [ 642.33   1587.05   1409.73   ...  394.       38.9      23.3115]\n",
      " [ 642.73   1591.61   1403.71   ...  393.       38.81     23.2616]\n",
      " ...\n",
      " [ 642.21   1582.66   1408.32   ...  394.       38.78     23.2137]\n",
      " [ 642.9    1589.83   1411.58   ...  393.       38.82     23.2623]\n",
      " [ 642.2    1590.2    1399.28   ...  393.       38.79     23.2749]]\n",
      "training with cv\n",
      "Train on 14927 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      "14927/14927 [==============================] - 1s 48us/step - loss: 4092.2056 - mean_squared_error: 4092.2056 - val_loss: 2980.5991 - val_mean_squared_error: 2980.5991\n",
      "Epoch 2/5\n",
      "14927/14927 [==============================] - 1s 38us/step - loss: 1907.8221 - mean_squared_error: 1907.8221 - val_loss: 1858.6086 - val_mean_squared_error: 1858.6086\n",
      "Epoch 3/5\n",
      "14927/14927 [==============================] - 1s 39us/step - loss: 1125.0913 - mean_squared_error: 1125.0913 - val_loss: 1394.8816 - val_mean_squared_error: 1394.8816\n",
      "Epoch 4/5\n",
      "14927/14927 [==============================] - 1s 39us/step - loss: 689.2348 - mean_squared_error: 689.2348 - val_loss: 955.8942 - val_mean_squared_error: 955.8942\n",
      "Epoch 5/5\n",
      "14927/14927 [==============================] - 1s 39us/step - loss: 446.6122 - mean_squared_error: 446.6122 - val_loss: 730.5660 - val_mean_squared_error: 730.5660\n",
      "20/20 [==============================] - 0s 41us/step\n",
      "Fetching model 4 to keras\n",
      "Reusing singleton instance\n",
      "<ann_framework.data_handlers.data_handler_CMAPSS.CMAPSSDataHandler object at 0x7f6887648550>\n",
      "[[-0.54362416 -0.34526854 -0.6456044  ... -0.6         0.6124031\n",
      "   0.41505195]\n",
      " [-0.59060403 -0.18107417 -0.57299843 ... -0.6         0.31782946\n",
      "   0.66835159]\n",
      " [-0.77181208 -0.11611253 -0.34419152 ... -0.6         0.27131783\n",
      "   0.61443415]\n",
      " ...\n",
      " [ 0.10067114  0.58516624  0.33398744 ...  0.8        -0.53488372\n",
      "  -0.89019938]\n",
      " [ 0.42281879  0.55498721  0.53649922 ...  0.2        -0.76744186\n",
      "  -0.52316765]\n",
      " [ 0.4295302   0.13452685  0.43877551 ...  0.4        -0.64341085\n",
      "  -0.55630441]]\n",
      "[[-0.10738255 -0.49616368 -0.26844584 ... -0.2         0.03875969\n",
      "   0.29458017]\n",
      " [-0.12751678 -0.18158568  0.02197802 ... -0.6         0.03875969\n",
      "   0.0322943 ]\n",
      " [ 0.06711409 -0.05473146  0.13814757 ...  0.2         0.2248062\n",
      "   0.06655434]\n",
      " ...\n",
      " [-0.10738255 -0.18925831 -0.07417582 ...  0.         -0.03875969\n",
      "   0.31030609]\n",
      " [-0.60402685 -0.53913043 -0.40816327 ... -0.6         0.25581395\n",
      "   0.30665543]\n",
      " [-0.30201342  0.22966752 -0.36381476 ...  0.4        -0.13178295\n",
      "  -0.18197136]]\n",
      "[[ 0.08053691  0.09207161  0.16522763 ... -0.2        -0.33333333\n",
      "  -0.60263971]\n",
      " [-0.24832215 -0.18107417  0.07849294 ...  0.          0.17829457\n",
      "   0.17186184]\n",
      " [ 0.02013423  0.05217391 -0.1577708  ... -0.2         0.03875969\n",
      "   0.03173266]\n",
      " ...\n",
      " [-0.32885906 -0.4056266   0.02315542 ...  0.         -0.00775194\n",
      "  -0.10278012]\n",
      " [ 0.13422819 -0.03887468  0.1510989  ... -0.2         0.05426357\n",
      "   0.0336984 ]\n",
      " [-0.33557047 -0.01994885 -0.33163265 ... -0.2         0.00775194\n",
      "   0.06908172]]\n",
      "Evaluating model 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (Dense)                   (None, 352)               118624    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               45184     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 163,937\n",
      "Trainable params: 163,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Using previously loaded data\n",
      "printing crossval\n",
      "[[ 642.82   1592.39   1411.94   ...  393.       38.57     23.0357]\n",
      " [ 642.33   1587.05   1409.73   ...  394.       38.9      23.3115]\n",
      " [ 642.73   1591.61   1403.71   ...  393.       38.81     23.2616]\n",
      " ...\n",
      " [ 642.21   1582.66   1408.32   ...  394.       38.78     23.2137]\n",
      " [ 642.9    1589.83   1411.58   ...  393.       38.82     23.2623]\n",
      " [ 642.2    1590.2    1399.28   ...  393.       38.79     23.2749]]\n",
      "training with cv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14927 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      "14927/14927 [==============================] - 0s 17us/step - loss: 7750.6310 - mean_squared_error: 7750.6310 - val_loss: 6808.3467 - val_mean_squared_error: 6808.3467\n",
      "Epoch 2/5\n",
      "14927/14927 [==============================] - 0s 8us/step - loss: 7109.6784 - mean_squared_error: 7109.6784 - val_loss: 6486.6543 - val_mean_squared_error: 6486.6543\n",
      "Epoch 3/5\n",
      "14927/14927 [==============================] - 0s 8us/step - loss: 6736.9674 - mean_squared_error: 6736.9674 - val_loss: 6202.3364 - val_mean_squared_error: 6202.3364\n",
      "Epoch 4/5\n",
      "14927/14927 [==============================] - 0s 8us/step - loss: 6397.5672 - mean_squared_error: 6397.5672 - val_loss: 5940.9556 - val_mean_squared_error: 5940.9556\n",
      "Epoch 5/5\n",
      "14927/14927 [==============================] - 0s 8us/step - loss: 6083.1286 - mean_squared_error: 6083.1286 - val_loss: 5693.6069 - val_mean_squared_error: 5693.6069\n",
      "20/20 [==============================] - 0s 16us/step\n",
      "metric_scaler 0.500000\n",
      "size scaler 0.500000\n",
      "Individual 0 score/normalized score/size/fitness 2426.220703125/0.26048633260205434/65577/3.7122036307812056\n",
      "metric_scaler 0.500000\n",
      "size scaler 0.500000\n",
      "Individual 1 score/normalized score/size/fitness 5720.892578125/0.6142121881025414/103697/5.579577610162097\n",
      "metric_scaler 0.500000\n",
      "size scaler 0.500000\n",
      "Individual 2 score/normalized score/size/fitness 3897.203857421875/0.4184154965435977/1618721/5.196700907094675\n",
      "metric_scaler 0.500000\n",
      "size scaler 0.500000\n",
      "Individual 3 score/normalized score/size/fitness 730.5659790039062/0.07843575497869804/966881/3.384892011934991\n",
      "metric_scaler 0.500000\n",
      "size scaler 0.500000\n",
      "Individual 4 score/normalized score/size/fitness 5693.60693359375/0.61128271945714/163937/5.663835521309549\n",
      "\n",
      "Generating offsprings\n",
      "Applying Mutation\n",
      "Launch new generation?: True\n",
      "Experiment 1 finished\n",
      "Experiment time: 0.20110586881637574 minutes\n",
      "Recompute globals fitness\n",
      "metric_scaler 0.500000\n",
      "size scaler 0.500000\n",
      "Recomputed fitness\n",
      "\n",
      "\n",
      "String Model\n",
      "[[<Layers.FullyConnected: 1>, 880, 1, 0, 0, 0, 0, 0], [<Layers.Dropout: 5>, 0, 0, 0, 0, 0, 0, 0.65], [<Layers.FullyConnected: 1>, 760, 1, 0, 0, 0, 0, 0], [<Layers.FullyConnected: 1>, 1, 4, 0, 0, 0, 0, 0]]\n",
      "<Individual(label = '0' fitness = 3655.8226, raw_score = 730.5660, normalized_score = 730.5660, raw_size = 966881)>\n",
      "Checksum vector: [8.000e+00 1.641e+03 6.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 6.500e-01]\n",
      "Global best index\n",
      "0\n",
      "Global best list\n",
      "\n",
      "\n",
      "\n",
      "String Model\n",
      "[[<Layers.FullyConnected: 1>, 880, 1, 0, 0, 0, 0, 0], [<Layers.Dropout: 5>, 0, 0, 0, 0, 0, 0, 0.65], [<Layers.FullyConnected: 1>, 760, 1, 0, 0, 0, 0, 0], [<Layers.FullyConnected: 1>, 1, 4, 0, 0, 0, 0, 0]]\n",
      "<Individual(label = '0' fitness = 3655.8226, raw_score = 730.5660, normalized_score = 730.5660, raw_size = 966881)>\n",
      "Checksum vector: [8.000e+00 1.641e+03 6.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 6.500e-01]\n",
      "Global best is\n",
      "\n",
      "\n",
      "\n",
      "String Model\n",
      "[[<Layers.FullyConnected: 1>, 880, 1, 0, 0, 0, 0, 0], [<Layers.Dropout: 5>, 0, 0, 0, 0, 0, 0, 0.65], [<Layers.FullyConnected: 1>, 760, 1, 0, 0, 0, 0, 0], [<Layers.FullyConnected: 1>, 1, 4, 0, 0, 0, 0, 0]]\n",
      "<Individual(label = '0' fitness = 3655.8226, raw_score = 730.5660, normalized_score = 730.5660, raw_size = 966881)>\n",
      "Checksum vector: [8.000e+00 1.641e+03 6.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 6.500e-01]\n",
      "Global time 0.20110586881637574\n",
      "[<nn_evolutionary.Individual object at 0x7f6887648828>]\n",
      "0\n",
      "Total experiment time 0.20110586881637574\n",
      "Avg experiment time 0.20110586881637574\n",
      "tipo problema\n",
      "1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (Dense)                   (None, 880)               296560    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 880)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 760)               669560    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 761       \n",
      "=================================================================\n",
      "Total params: 966,881\n",
      "Trainable params: 966,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "<ann_framework.data_handlers.data_handler_CMAPSS.CMAPSSDataHandler object at 0x7f6887648550>\n",
      "No data scaling used\n",
      "Loading data for the first time\n",
      "Reloading data due to parameter change\n",
      "Loading data for dataset 1 with window_size of 24, stride of 1 and maxRUL of 129. Cros-Validation ratio 0\n",
      "Loading data from file and computing dataframes\n",
      "Printing shapes\n",
      "\n",
      "Training data (X, y)\n",
      "(18331, 336)\n",
      "(18331, 1)\n",
      "Testing data (X, y)\n",
      "(100, 336)\n",
      "(100, 1)\n",
      "Printing first 5 elements\n",
      "\n",
      "Training data (X, y)\n",
      "[[-0.59060403 -0.0455243  -0.27982732 ... -0.45454545  0.33333333\n",
      "   0.33501825]\n",
      " [-0.36912752  0.0629156  -0.18014129 ... -0.27272727  0.25581395\n",
      "   0.50126369]\n",
      " [-0.23489933 -0.13299233 -0.13854003 ... -0.09090909  0.11627907\n",
      "   0.46222971]\n",
      " [-0.23489933 -0.39897698 -0.2299843  ... -0.27272727  0.31782946\n",
      "   0.55293457]\n",
      " [-0.22147651 -0.39590793 -0.05926217 ... -0.81818182  0.34883721\n",
      "   0.09491716]]\n",
      "[[129.]\n",
      " [129.]\n",
      " [129.]\n",
      " [129.]\n",
      " [129.]]\n",
      "Testing data (X, y)\n",
      "[[-0.10738255 -0.49616368 -0.26844584 ... -0.27272727  0.03875969\n",
      "   0.29458017]\n",
      " [-0.12751678 -0.18158568  0.02197802 ... -0.63636364  0.03875969\n",
      "   0.0322943 ]\n",
      " [ 0.06711409 -0.05473146  0.13814757 ...  0.09090909  0.2248062\n",
      "   0.06655434]\n",
      " [-0.1409396   0.15294118  0.00353218 ...  0.09090909 -0.31782946\n",
      "   0.02190396]\n",
      " [-0.01342282  0.03273657  0.2032967  ... -0.09090909 -0.05426357\n",
      "   0.45324347]]\n",
      "[[112.]\n",
      " [ 98.]\n",
      " [ 69.]\n",
      " [ 82.]\n",
      " [ 91.]]\n",
      "training without cv\n",
      "Epoch 1/100\n",
      "18331/18331 [==============================] - 1s 43us/step - loss: 1383.9377 - mean_squared_error: 1383.9377\n",
      "Epoch 2/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 515.3866 - mean_squared_error: 515.3866\n",
      "Epoch 3/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 302.4075 - mean_squared_error: 302.4075\n",
      "Epoch 4/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 272.8852 - mean_squared_error: 272.8852\n",
      "Epoch 5/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 260.3576 - mean_squared_error: 260.3576\n",
      "Epoch 6/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 261.9155 - mean_squared_error: 261.9155\n",
      "Epoch 7/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 253.2683 - mean_squared_error: 253.2683\n",
      "Epoch 8/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 243.4397 - mean_squared_error: 243.4397\n",
      "Epoch 9/100\n",
      "18331/18331 [==============================] - 1s 39us/step - loss: 237.2348 - mean_squared_error: 237.2348\n",
      "Epoch 10/100\n",
      "18331/18331 [==============================] - 1s 38us/step - loss: 239.0240 - mean_squared_error: 239.0240\n",
      "Epoch 11/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 237.4369 - mean_squared_error: 237.4369\n",
      "Epoch 12/100\n",
      "18331/18331 [==============================] - 1s 38us/step - loss: 236.9155 - mean_squared_error: 236.9155\n",
      "Epoch 13/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 226.0717 - mean_squared_error: 226.0717\n",
      "Epoch 14/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 223.4992 - mean_squared_error: 223.4992\n",
      "Epoch 15/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 229.9226 - mean_squared_error: 229.9226\n",
      "Epoch 16/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 223.4497 - mean_squared_error: 223.4497\n",
      "Epoch 17/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 222.5544 - mean_squared_error: 222.5544\n",
      "Epoch 18/100\n",
      "18331/18331 [==============================] - 1s 35us/step - loss: 216.5449 - mean_squared_error: 216.5449\n",
      "Epoch 19/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 211.0983 - mean_squared_error: 211.0983\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18331/18331 [==============================] - 1s 35us/step - loss: 215.3373 - mean_squared_error: 215.3373\n",
      "Epoch 21/100\n",
      "18331/18331 [==============================] - 1s 35us/step - loss: 213.5660 - mean_squared_error: 213.5660\n",
      "Epoch 22/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 204.4086 - mean_squared_error: 204.4086\n",
      "Epoch 23/100\n",
      "18331/18331 [==============================] - 1s 35us/step - loss: 217.1023 - mean_squared_error: 217.1023\n",
      "Epoch 24/100\n",
      "18331/18331 [==============================] - 1s 38us/step - loss: 209.0019 - mean_squared_error: 209.0019\n",
      "Epoch 25/100\n",
      "18331/18331 [==============================] - 1s 38us/step - loss: 207.0033 - mean_squared_error: 207.0033\n",
      "Epoch 26/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 205.2042 - mean_squared_error: 205.2042\n",
      "Epoch 27/100\n",
      "18331/18331 [==============================] - 1s 39us/step - loss: 199.0473 - mean_squared_error: 199.0473\n",
      "Epoch 28/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 198.6334 - mean_squared_error: 198.6334\n",
      "Epoch 29/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 195.6950 - mean_squared_error: 195.6950\n",
      "Epoch 30/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 197.6741 - mean_squared_error: 197.6741\n",
      "Epoch 31/100\n",
      "18331/18331 [==============================] - 1s 35us/step - loss: 198.4088 - mean_squared_error: 198.4088\n",
      "Epoch 32/100\n",
      "18331/18331 [==============================] - 1s 35us/step - loss: 202.9264 - mean_squared_error: 202.9264\n",
      "Epoch 33/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 195.7319 - mean_squared_error: 195.7319\n",
      "Epoch 34/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 191.4873 - mean_squared_error: 191.4873\n",
      "Epoch 35/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 198.0818 - mean_squared_error: 198.0818\n",
      "Epoch 36/100\n",
      "18331/18331 [==============================] - 1s 35us/step - loss: 193.7582 - mean_squared_error: 193.7582\n",
      "Epoch 37/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 189.9615 - mean_squared_error: 189.9615\n",
      "Epoch 38/100\n",
      "18331/18331 [==============================] - 1s 35us/step - loss: 198.6640 - mean_squared_error: 198.6640\n",
      "Epoch 39/100\n",
      "18331/18331 [==============================] - 1s 35us/step - loss: 194.2714 - mean_squared_error: 194.2714\n",
      "Epoch 40/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 191.0553 - mean_squared_error: 191.0553\n",
      "Epoch 41/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 186.6339 - mean_squared_error: 186.6339\n",
      "Epoch 42/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 190.5397 - mean_squared_error: 190.5397\n",
      "Epoch 43/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 193.8005 - mean_squared_error: 193.8005\n",
      "Epoch 44/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 185.6960 - mean_squared_error: 185.6960\n",
      "Epoch 45/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 186.1907 - mean_squared_error: 186.1907\n",
      "Epoch 46/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 189.4168 - mean_squared_error: 189.4168\n",
      "Epoch 47/100\n",
      "18331/18331 [==============================] - 1s 35us/step - loss: 188.5650 - mean_squared_error: 188.5650\n",
      "Epoch 48/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 184.4303 - mean_squared_error: 184.4303\n",
      "Epoch 49/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 185.4959 - mean_squared_error: 185.4959\n",
      "Epoch 50/100\n",
      "18331/18331 [==============================] - 1s 35us/step - loss: 184.1609 - mean_squared_error: 184.1609\n",
      "Epoch 51/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 183.5289 - mean_squared_error: 183.5289\n",
      "Epoch 52/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 182.0569 - mean_squared_error: 182.0569\n",
      "Epoch 53/100\n",
      "18331/18331 [==============================] - 1s 35us/step - loss: 188.0386 - mean_squared_error: 188.0386\n",
      "Epoch 54/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 190.5052 - mean_squared_error: 190.5052\n",
      "Epoch 55/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 182.7488 - mean_squared_error: 182.7488\n",
      "Epoch 56/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 181.2683 - mean_squared_error: 181.2683\n",
      "Epoch 57/100\n",
      "18331/18331 [==============================] - 1s 38us/step - loss: 183.6173 - mean_squared_error: 183.6173\n",
      "Epoch 58/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 187.8024 - mean_squared_error: 187.8024\n",
      "Epoch 59/100\n",
      "18331/18331 [==============================] - 1s 38us/step - loss: 181.5457 - mean_squared_error: 181.5457\n",
      "Epoch 60/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 186.6215 - mean_squared_error: 186.6215\n",
      "Epoch 61/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 185.9325 - mean_squared_error: 185.9325\n",
      "Epoch 62/100\n",
      "18331/18331 [==============================] - 1s 35us/step - loss: 186.2302 - mean_squared_error: 186.2302\n",
      "Epoch 63/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 185.8406 - mean_squared_error: 185.8406\n",
      "Epoch 64/100\n",
      "18331/18331 [==============================] - 1s 35us/step - loss: 183.8288 - mean_squared_error: 183.8288\n",
      "Epoch 65/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 184.3229 - mean_squared_error: 184.3229\n",
      "Epoch 66/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 181.7971 - mean_squared_error: 181.7971\n",
      "Epoch 67/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 183.4672 - mean_squared_error: 183.4672\n",
      "Epoch 68/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 182.1685 - mean_squared_error: 182.1685\n",
      "Epoch 69/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 186.6357 - mean_squared_error: 186.6357\n",
      "Epoch 70/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 184.7060 - mean_squared_error: 184.7060\n",
      "Epoch 71/100\n",
      "18331/18331 [==============================] - 1s 35us/step - loss: 181.9829 - mean_squared_error: 181.9829\n",
      "Epoch 72/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 185.5178 - mean_squared_error: 185.5178\n",
      "Epoch 73/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 187.4342 - mean_squared_error: 187.4342\n",
      "Epoch 74/100\n",
      "18331/18331 [==============================] - 1s 38us/step - loss: 183.9500 - mean_squared_error: 183.9500\n",
      "Epoch 75/100\n",
      "18331/18331 [==============================] - 1s 38us/step - loss: 184.1985 - mean_squared_error: 184.1985\n",
      "Epoch 76/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 186.9338 - mean_squared_error: 186.9338\n",
      "Epoch 77/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 182.1465 - mean_squared_error: 182.1465\n",
      "Epoch 78/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 184.3036 - mean_squared_error: 184.3036\n",
      "Epoch 79/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 185.7679 - mean_squared_error: 185.7679\n",
      "Epoch 80/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 185.1092 - mean_squared_error: 185.1092\n",
      "Epoch 81/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 187.0142 - mean_squared_error: 187.0142\n",
      "Epoch 82/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 185.8609 - mean_squared_error: 185.8609\n",
      "Epoch 83/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 184.2060 - mean_squared_error: 184.2060\n",
      "Epoch 84/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 181.7296 - mean_squared_error: 181.7296\n",
      "Epoch 85/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 184.7950 - mean_squared_error: 184.7950\n",
      "Epoch 86/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 185.0219 - mean_squared_error: 185.0219\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18331/18331 [==============================] - 1s 37us/step - loss: 185.0428 - mean_squared_error: 185.0428\n",
      "Epoch 88/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 183.0320 - mean_squared_error: 183.0320\n",
      "Epoch 89/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 181.4418 - mean_squared_error: 181.4418\n",
      "Epoch 90/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 180.7241 - mean_squared_error: 180.7241\n",
      "Epoch 91/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 184.5460 - mean_squared_error: 184.5460\n",
      "Epoch 92/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 184.9212 - mean_squared_error: 184.9212\n",
      "Epoch 93/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 181.8714 - mean_squared_error: 181.8714\n",
      "Epoch 94/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 184.0756 - mean_squared_error: 184.0756\n",
      "Epoch 95/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 184.8716 - mean_squared_error: 184.8716\n",
      "Epoch 96/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 179.9211 - mean_squared_error: 179.9211\n",
      "Epoch 97/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 183.4288 - mean_squared_error: 183.4288\n",
      "Epoch 98/100\n",
      "18331/18331 [==============================] - 1s 37us/step - loss: 182.7852 - mean_squared_error: 182.7852\n",
      "Epoch 99/100\n",
      "18331/18331 [==============================] - 1s 38us/step - loss: 182.3421 - mean_squared_error: 182.3421\n",
      "Epoch 100/100\n",
      "18331/18331 [==============================] - 1s 36us/step - loss: 186.6429 - mean_squared_error: 186.6429\n",
      "100/100 [==============================] - 0s 204us/step\n",
      "Saved models for dataset 1 to disk\n",
      "Running for alpha=0.6\n",
      "Launching experiment 1\n",
      "\n",
      "Generation 1\n",
      "launch new\n",
      "True\n",
      "gen similar\n",
      "False\n",
      "Fetching model 0 to keras\n",
      "Reusing singleton instance\n",
      "<ann_framework.data_handlers.data_handler_CMAPSS.CMAPSSDataHandler object at 0x7f6887648550>\n",
      "[[-0.54362416 -0.34526854 -0.6456044  ... -0.6         0.6124031\n",
      "   0.41505195]\n",
      " [-0.59060403 -0.18107417 -0.57299843 ... -0.6         0.31782946\n",
      "   0.66835159]\n",
      " [-0.77181208 -0.11611253 -0.34419152 ... -0.6         0.27131783\n",
      "   0.61443415]\n",
      " ...\n",
      " [ 0.10067114  0.58516624  0.33398744 ...  0.8        -0.53488372\n",
      "  -0.89019938]\n",
      " [ 0.42281879  0.55498721  0.53649922 ...  0.2        -0.76744186\n",
      "  -0.52316765]\n",
      " [ 0.4295302   0.13452685  0.43877551 ...  0.4        -0.64341085\n",
      "  -0.55630441]]\n",
      "[[-0.10738255 -0.49616368 -0.26844584 ... -0.2         0.03875969\n",
      "   0.29458017]\n",
      " [-0.12751678 -0.18158568  0.02197802 ... -0.6         0.03875969\n",
      "   0.0322943 ]\n",
      " [ 0.06711409 -0.05473146  0.13814757 ...  0.2         0.2248062\n",
      "   0.06655434]\n",
      " ...\n",
      " [-0.10738255 -0.18925831 -0.07417582 ...  0.         -0.03875969\n",
      "   0.31030609]\n",
      " [-0.60402685 -0.53913043 -0.40816327 ... -0.6         0.25581395\n",
      "   0.30665543]\n",
      " [-0.30201342  0.22966752 -0.36381476 ...  0.4        -0.13178295\n",
      "  -0.18197136]]\n",
      "[[ 0.08053691  0.09207161  0.16522763 ... -0.2        -0.33333333\n",
      "  -0.60263971]\n",
      " [-0.24832215 -0.18107417  0.07849294 ...  0.          0.17829457\n",
      "   0.17186184]\n",
      " [ 0.02013423  0.05217391 -0.1577708  ... -0.2         0.03875969\n",
      "   0.03173266]\n",
      " ...\n",
      " [-0.32885906 -0.4056266   0.02315542 ...  0.         -0.00775194\n",
      "  -0.10278012]\n",
      " [ 0.13422819 -0.03887468  0.1510989  ... -0.2         0.05426357\n",
      "   0.0336984 ]\n",
      " [-0.33557047 -0.01994885 -0.33163265 ... -0.2         0.00775194\n",
      "   0.06908172]]\n",
      "Evaluating model 0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (Dense)                   (None, 616)               207592    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1008)              621936    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1009      \n",
      "=================================================================\n",
      "Total params: 830,537\n",
      "Trainable params: 830,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Using previously loaded data\n",
      "printing crossval\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=nan.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0f31e3036862>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                                                             \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                                                             \u001b[0msize_scaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_scaler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                                                                             total_experiments=experiments)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_best_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-aa8cf72a1e45>\u001b[0m in \u001b[0;36mcmapss_test\u001b[0;34m(dhandler_cmapss, input_shape, size_scaler, total_experiments)\u001b[0m\n\u001b[1;32m     48\u001b[0m         best = automatic_model_selection.run_experiment(config, dhandler_cmapss, count_experiments + 1, unroll=unroll,\n\u001b[1;32m     49\u001b[0m                                                         \u001b[0mlearningRate_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearningRate_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                                                         tModel_scaler=scaler)\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/controlslab/DATA/Projects/automatic_model_selection/code/automatic_model_selection.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(configuration, data_handler, experiment_number, unroll, learningRate_scheduler, tModel_scaler)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;31m#Assess the fitness of the inidividuals in the population\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \t\tbest_model, worst_model, worst_index = evaluate_population(population, configuration, data_handler, tModel_scaler,\n\u001b[0;32m--> 442\u001b[0;31m \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   unroll, learningRate_scheduler)\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                 \u001b[0;31m#Save worst and best models. Also append best model to elite archive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/controlslab/DATA/Projects/automatic_model_selection/code/automatic_model_selection.py\u001b[0m in \u001b[0;36mevaluate_population\u001b[0;34m(population, configuration, data_handler, tModel_scaler, unroll, learningRate_scheduler)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \t\tevaluate_individual(individual, configuration, data_handler, tModel_scaler, i, unroll,\n\u001b[0;32m--> 236\u001b[0;31m \t\t\t\t\t\t\tlearningRate_scheduler=learningRate_scheduler)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0mraw_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindividual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/controlslab/DATA/Projects/automatic_model_selection/code/automatic_model_selection.py\u001b[0m in \u001b[0;36mevaluate_individual\u001b[0;34m(individual, configuration, data_handler, tModel_scaler, ind_index, unroll, learningRate_scheduler)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \tindividual.compute_raw_scores(epochs=configuration.epochs, cross_validation_ratio=configuration.cross_val,\n\u001b[0;32m--> 213\u001b[0;31m \t\t\t\t      verbose=configuration.verbose_training, unroll=unroll, learningRate_scheduler=learningRate_scheduler)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mindividual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindividual_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mind_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/controlslab/DATA/Projects/automatic_model_selection/code/nn_evolutionary.py\u001b[0m in \u001b[0;36mcompute_raw_scores\u001b[0;34m(self, epochs, cross_validation_ratio, verbose, unroll, learningRate_scheduler)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainable_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_validation_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munroll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearningRate_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearningRate_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mmetric_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/controlslab/DATA/Projects/automatic_model_selection/code/nn_evolutionary.py\u001b[0m in \u001b[0;36mpartial_run\u001b[0;34m(self, cross_validation_ratio, epochs, verbose, unroll, learningRate_scheduler)\u001b[0m\n\u001b[1;32m     88\u001b[0m \t\t\"\"\"\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_validation_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_validation_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munroll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \t\t\"\"\"\n",
      "\u001b[0;32m/media/controlslab/DATA/Projects/ann_framework/tunable_model/tunable_model.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, unroll, cross_validation_ratio, verbose, reload_data, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"printing crossval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_crossVal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                                         \u001b[0mX_crossVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_crossVal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_X_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES,\n\u001b[0;32m--> 385\u001b[0;31m                         force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    543\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    546\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=nan.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "#alphas = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "alphas = [0.5, 0.6]\n",
    "experiments = 1\n",
    "problem_type = 1\n",
    "\n",
    "global_best_list = []\n",
    "global_best_index = 0\n",
    "total_experiment_time = []\n",
    "total_experiment_time = 0\n",
    "avg_experiment_time = 0\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "dhandler_cmapss, input_shape = cmapss_dhandler()\n",
    "\n",
    "for size_scaler in alphas:\n",
    "\n",
    "    print(\"Running for alpha={}\".format(size_scaler))\n",
    "    \n",
    "    global_best_list, global_best_index, total_experiment_time = cmapss_test(dhandler_cmapss=dhandler_cmapss, \n",
    "                                                                            input_shape=input_shape,\n",
    "                                                                            size_scaler=size_scaler, \n",
    "                                                                            total_experiments=experiments)\n",
    "    \n",
    "    print(global_best_list)\n",
    "    print(global_best_index)\n",
    "    \n",
    "    avg_experiment_time = total_experiment_time/experiments\n",
    "    \n",
    "    print(\"Total experiment time {}\".format(total_experiment_time))\n",
    "    print(\"Avg experiment time {}\".format(avg_experiment_time))\n",
    "    \n",
    "    save_best_models(global_best_list, global_best_index, \n",
    "                     'best_models/cmapss/yulin/alpha{}/'.format(size_scaler), input_shape=input_shape, \n",
    "                     data_handler=dhandler_cmapss, problem_type=problem_type, train_epochs=5, data_scaler=scaler)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "save_best_models(global_best_list, global_best_index, 'best_models/mnist/alpha{}/scalarized_version/'.format(size_scaler), \n",
    "                 input_shape=input_shape, data_handler=dHandler_mnist, problem_type=problem_type, train_epochs=100)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "save_best_models(global_best_list, global_best_index, 'best_models/cifar10/alpha{}/version2'.format(size_scaler), \n",
    "                 input_shape=input_shape, data_handler=dHandler_cifar, problem_type=problem_type, train_epochs=100)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "save_best_models(global_best_list, global_best_index, 'best_models/cmapss/alpha{}/'.format(size_scaler), \n",
    "                 input_shape=input_shape, data_handler=dhandler_cmaps, problem_type=problem_type, train_epochs=100, \n",
    "                 data_scaler=scaler, metrics=['rhs', 'rmse'], round=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
